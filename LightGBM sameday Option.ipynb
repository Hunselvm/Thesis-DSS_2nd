{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e9b7fe",
   "metadata": {},
   "source": [
    "Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fa0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         impl_volatility  cp_flag  stock_price  moneyness  time_to_expiry  \\\n",
      "0               0.210270        1     115.5450   0.970966              24   \n",
      "1               0.208124        1     115.5450   0.962875              24   \n",
      "2               0.205474        1     115.5450   0.954917              24   \n",
      "3               0.278442        0     115.5450   1.100429              24   \n",
      "4               0.242212        0     115.5450   1.050409              24   \n",
      "...                  ...      ...          ...        ...             ...   \n",
      "1634738         0.231520        1     111.0158   0.965355             141   \n",
      "1634739         0.259238        0     111.0158   1.110158             141   \n",
      "1634740         0.247787        0     111.0158   1.057293             141   \n",
      "1634741         0.236913        0     111.0158   1.009235             141   \n",
      "1634742         0.265734        0     111.0158   1.138624             141   \n",
      "\n",
      "         strike_price     delta     gamma       vega      theta      rf  \\\n",
      "0               119.0  0.329402  0.057909  10.772000 -16.612410  0.0245   \n",
      "1               120.0  0.273000  0.053728   9.892975 -15.153390  0.0245   \n",
      "2               121.0  0.220664  0.048533   8.819048 -13.381580  0.0245   \n",
      "3               105.0 -0.075725  0.017194   4.246497  -9.138635  0.0245   \n",
      "4               110.0 -0.186759  0.037192   7.995862 -15.079300  0.0245   \n",
      "...               ...       ...       ...        ...        ...     ...   \n",
      "1634738         115.0  0.467404  0.024692  27.284490 -10.731930  0.0409   \n",
      "1634739         100.0 -0.211384  0.016458  19.826610  -5.427654  0.0409   \n",
      "1634740         105.0 -0.306024  0.021056  23.978630  -5.936885  0.0409   \n",
      "1634741         110.0 -0.421900  0.024836  26.610920  -5.760072  0.0409   \n",
      "1634742          97.5 -0.173115  0.014163  17.598220  -5.030665  0.0409   \n",
      "\n",
      "          iv_ahbs  iv_ahbs_error     iv_bs  iv_bs_error     iv_cw  iv_cw_error  \n",
      "0        0.346996       0.136726  0.353798     0.143528  0.316948     0.106678  \n",
      "1        0.346099       0.137975  0.353798     0.145674  0.318967     0.110843  \n",
      "2        0.345399       0.139925  0.353798     0.148324  0.321854     0.116380  \n",
      "3        0.386757       0.108315  0.353798     0.075356  0.388674     0.110232  \n",
      "4        0.365726       0.123514  0.353798     0.111586  0.342023     0.099811  \n",
      "...           ...            ...       ...          ...       ...          ...  \n",
      "1634738  0.294146       0.062626  0.353798     0.122278  0.296576     0.065056  \n",
      "1634739  0.304049       0.044811  0.353798     0.094560  0.321908     0.062670  \n",
      "1634740  0.293500       0.045713  0.353798     0.106011  0.307279     0.059492  \n",
      "1634741  0.290830       0.053917  0.353798     0.116885  0.298955     0.062042  \n",
      "1634742  0.313032       0.047298  0.353798     0.088064  0.331627     0.065893  \n",
      "\n",
      "[1634743 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_train_sameday_real.csv')\n",
    "test_data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_test_sameday_real.csv')\n",
    "columns_to_remove = ['volume', 'open_interest', 'option_price']\n",
    "test_data = test_data.drop(columns=columns_to_remove)\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "test_data_with_all_predictions = test_data.copy() \n",
    "\n",
    "option_columns = [\n",
    "    'impl_volatility',\n",
    "    'cp_flag',\n",
    "    'stock_price',\n",
    "    'moneyness',\n",
    "    'time_to_expiry',\n",
    "    'strike_price',\n",
    "    'delta',\n",
    "    'gamma',\n",
    "    'vega',\n",
    "    'theta',\n",
    "    'rf',\n",
    "    'iv_ahbs',\n",
    "    'iv_ahbs_error',\n",
    "    'iv_bs',\n",
    "    'iv_bs_error',\n",
    "    'iv_cw',\n",
    "    'iv_cw_error'\n",
    "]\n",
    "\n",
    "option_only = data[option_columns]\n",
    "print(option_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 1: LIGHTGBM MODEL DEFINITION\n",
    "###########################################\n",
    "\n",
    "def create_lgb_model(model_type):\n",
    "\n",
    "    if model_type == 'LGB1':\n",
    "        # Standard LightGBM configuration\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    elif model_type == 'LGB2':\n",
    "        # More complex LightGBM configuration with different hyperparameters\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 63,\n",
    "            'learning_rate': 0.01,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq': 4,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'max_depth': 10,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Choose from 'LGB1' or 'LGB2'.\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "def train_and_evaluate_model(params, X_train, y_train, X_test, y_test, num_boost_round=100):\n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return model, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66054835",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 2: DATA PREPARATION\n",
    "###########################################\n",
    "\n",
    "def prepare_data(option_only):\n",
    "\n",
    "    # Prepare features and target variables\n",
    "    feature_columns = [col for col in option_only.columns if col not in \n",
    "                      ['iv_bs_error', 'iv_ahbs', \"iv_ahbs_error\", \"iv_bs\", \n",
    "                       \"iv_cw\", \"iv_cw_error\", \"impl_volatility\"]]\n",
    "    \n",
    "    # Instead of using train_test_split, use your predefined sets\n",
    "    X_train = option_only[feature_columns]  # Features from your training set\n",
    "    X_test = test_data[feature_columns]  # Features from your test set\n",
    "\n",
    "    # Target variables for each error type\n",
    "    y_bs_train = option_only['iv_bs_error']  \n",
    "    y_bs_test = test_data['iv_bs_error']  \n",
    "\n",
    "    y_ahbs_train = option_only['iv_ahbs_error']  \n",
    "    y_ahbs_test = test_data['iv_ahbs_error'] \n",
    "\n",
    "    y_cw_train = option_only['iv_cw_error']  \n",
    "    y_cw_test = test_data['iv_cw_error'] \n",
    "    \n",
    "    # Initialize a StandardScaler to normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on the training data and transform both training and testing data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create dictionaries to store multiple target variables\n",
    "    y_train_dict = {\n",
    "        'bs': y_bs_train,\n",
    "        'ahbs': y_ahbs_train,\n",
    "        'cw': y_cw_train\n",
    "    }\n",
    "    \n",
    "    y_test_dict = {\n",
    "        'bs': y_bs_test,\n",
    "        'ahbs': y_ahbs_test,\n",
    "        'cw': y_cw_test\n",
    "    }\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_dict, y_test_dict, scaler, feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a205ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 3: PREDICTION FUNCTION\n",
    "###########################################\n",
    "\n",
    "def predict_and_add_to_test_data(models, test_data, scaler, feature_columns, error_type='bs'):\n",
    "\n",
    "    # Create a copy of the test data to avoid modifying the original\n",
    "    result_df = test_data.copy()\n",
    "    \n",
    "    # Extract features from test data\n",
    "    X_test = test_data[feature_columns].values\n",
    "    \n",
    "    # Scale the features using the pre-fitted scaler\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Original value column name\n",
    "    original_column = f'iv_{error_type}'\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Add predictions to the dataframe\n",
    "        column_name = f'iv_{error_type}_pred_{model_name}'\n",
    "        result_df[column_name] = predictions\n",
    "        \n",
    "        # Calculate corrected value by adding the error prediction to the original value\n",
    "        result_df[f'iv_{error_type}_corrected_{model_name}'] = result_df[original_column] - predictions\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefb6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 4: FEATURE IMPORTANCE ANALYSIS\n",
    "###########################################\n",
    "\n",
    "def analyze_feature_importance(model, feature_columns):\n",
    "    \"\"\"\n",
    "    Extract and display feature importance from a LightGBM model.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained LightGBM model\n",
    "    feature_columns (list): List of feature column names\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with feature importance scores\n",
    "    \"\"\"\n",
    "    # Get feature importance\n",
    "    importance = model.feature_importance(importance_type='split')\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1646d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training models for bs error correction ===\n",
      "\n",
      "Training LGB1_bs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000226269\n",
      "LGB1_bs Test MSE: 0.0002262685848857054\n",
      "\n",
      "Training LGB2_bs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000609212\n",
      "LGB2_bs Test MSE: 0.0006092122491386029\n",
      "\n",
      "=== Training models for ahbs error correction ===\n",
      "\n",
      "Training LGB1_ahbs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000223554\n",
      "LGB1_ahbs Test MSE: 0.00022355414000430285\n",
      "\n",
      "Training LGB2_ahbs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000587698\n",
      "LGB2_ahbs Test MSE: 0.0005876983208977582\n",
      "\n",
      "=== Training models for cw error correction ===\n",
      "\n",
      "Training LGB1_cw...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000252201\n",
      "LGB1_cw Test MSE: 0.00025220053630735964\n",
      "\n",
      "Training LGB2_cw...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000600752\n",
      "LGB2_cw Test MSE: 0.0006007517612477313\n",
      "\n",
      "=== Making predictions for bs error correction ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of test data with bs predictions:\n",
      "      iv_bs  iv_bs_pred_LGB1  iv_bs_corrected_LGB1  iv_bs_pred_LGB2  \\\n",
      "0  0.353798         0.152401              0.201398         0.147649   \n",
      "1  0.353798         0.149811              0.203987         0.149893   \n",
      "2  0.353798         0.152491              0.201307         0.152401   \n",
      "3  0.353798         0.148667              0.205131         0.148020   \n",
      "4  0.353798         0.127547              0.226251         0.121123   \n",
      "\n",
      "   iv_bs_corrected_LGB2  \n",
      "0              0.206149  \n",
      "1              0.203905  \n",
      "2              0.201397  \n",
      "3              0.205779  \n",
      "4              0.232675  \n",
      "\n",
      "=== Making predictions for ahbs error correction ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of test data with ahbs predictions:\n",
      "    iv_ahbs  iv_ahbs_pred_LGB1  iv_ahbs_corrected_LGB1  iv_ahbs_pred_LGB2  \\\n",
      "0  0.350970           0.150908                0.200063           0.149187   \n",
      "1  0.349422           0.149656                0.199765           0.152786   \n",
      "2  0.348100           0.149932                0.198168           0.154409   \n",
      "3  0.344389           0.137132                0.207257           0.138896   \n",
      "4  0.359680           0.135478                0.224202           0.132251   \n",
      "\n",
      "   iv_ahbs_corrected_LGB2  \n",
      "0                0.201783  \n",
      "1                0.196636  \n",
      "2                0.193691  \n",
      "3                0.205492  \n",
      "4                0.227429  \n",
      "\n",
      "=== Making predictions for cw error correction ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of test data with cw predictions:\n",
      "      iv_cw  iv_cw_pred_LGB1  iv_cw_corrected_LGB1  iv_cw_pred_LGB2  \\\n",
      "0  0.316475         0.118035              0.198440         0.113300   \n",
      "1  0.315673         0.114406              0.201267         0.116041   \n",
      "2  0.315839         0.118137              0.197703         0.117249   \n",
      "3  0.341157         0.133201              0.207955         0.133796   \n",
      "4  0.329592         0.104031              0.225561         0.099750   \n",
      "\n",
      "   iv_cw_corrected_LGB2  \n",
      "0              0.203175  \n",
      "1              0.199632  \n",
      "2              0.198590  \n",
      "3              0.207361  \n",
      "4              0.229842  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "# PART 5: COMPLETE WORKFLOW\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Prepare data\n",
    "    X_train_scaled, X_test_scaled, y_train_dict, y_test_dict, scaler, feature_columns = prepare_data(option_only)\n",
    "    \n",
    "    # Step 2: Train models for each target variable\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Dictionary to store all models\n",
    "    all_models = {\n",
    "        'bs': {},\n",
    "        'ahbs': {},\n",
    "        'cw': {}\n",
    "    }\n",
    "    \n",
    "    # Train models for each error type\n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        print(f\"\\n=== Training models for {error_type} error correction ===\")\n",
    "        \n",
    "        # Get the appropriate training and test targets\n",
    "        y_train = y_train_dict[error_type]\n",
    "        y_test = y_test_dict[error_type]\n",
    "        \n",
    "        # Train each model configuration\n",
    "        for lgb_type in ['LGB1', 'LGB2']:\n",
    "            model_name = f\"{lgb_type}_{error_type}\"\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            # Create model parameters\n",
    "            params = create_lgb_model(lgb_type)\n",
    "            \n",
    "            # Train and evaluate model\n",
    "            model, mse = train_and_evaluate_model(\n",
    "                params, X_train_scaled, y_train, X_test_scaled, y_test, num_boost_round=500\n",
    "            )\n",
    "            \n",
    "            # Store model and results\n",
    "            all_models[error_type][lgb_type] = model\n",
    "            results[model_name] = {\n",
    "                'mse': mse,\n",
    "                'feature_importance': analyze_feature_importance(model, feature_columns)\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_name} Test MSE: {mse}\")\n",
    "            \n",
    "            # Save feature importance\n",
    "            results[model_name]['feature_importance'].to_csv(f\"{model_name}_feature_importance_sameday.csv\", index=False)\n",
    "            \n",
    "            # Save model (using LightGBM's save_model method)\n",
    "            model.save_model(f\"{model_name}_model_sameday.txt\")\n",
    "    \n",
    "    # Step 3: Generate predictions on test data for each error type\n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        print(f\"\\n=== Making predictions for {error_type} error correction ===\")\n",
    "        \n",
    "        # Extract the models for this error type\n",
    "        current_models = all_models[error_type]\n",
    "        \n",
    "        # Make predictions - use the accumulated DataFrame\n",
    "        test_data_with_all_predictions = predict_and_add_to_test_data(\n",
    "            current_models, test_data_with_all_predictions, scaler, feature_columns, error_type\n",
    "        )\n",
    "        \n",
    "        # Save intermediate results if desired\n",
    "        test_data_with_all_predictions.to_csv(f'test_data_with_{error_type}_predictions_sameday.csv', index=False)\n",
    "        \n",
    "        # Display sample results from accumulated DataFrame\n",
    "        print(f\"\\nSample of test data with {error_type} predictions:\")\n",
    "        display_columns = ['iv_' + error_type]\n",
    "        for lgb_type in ['LGB1', 'LGB2']:\n",
    "            display_columns.extend([\n",
    "                f'iv_{error_type}_pred_{lgb_type}', \n",
    "                f'iv_{error_type}_corrected_{lgb_type}'\n",
    "            ])\n",
    "        \n",
    "        print(test_data_with_all_predictions[display_columns].head(5))\n",
    "\n",
    "    # Final save with all predictions\n",
    "    test_data_with_all_predictions.to_csv('test_data_with_all_predictions_sameday.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2b159",
   "metadata": {},
   "source": [
    "Calculate IVRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b089598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IVRMSE Results ===\n",
      "Model           IVRMSE     Improvement \n",
      "----------------------------------------\n",
      "BS              0.150568  (baseline)  \n",
      "bs_LGB1         0.015042  90.01%\n",
      "bs_LGB2         0.024682  83.61%\n",
      "----------------------------------------\n",
      "AHBS            0.134068  (baseline)  \n",
      "ahbs_LGB1       0.014952  88.85%\n",
      "ahbs_LGB2       0.024242  81.92%\n",
      "----------------------------------------\n",
      "CW              0.128764  (baseline)  \n",
      "cw_LGB1         0.015881  87.67%\n",
      "cw_LGB2         0.024510  80.97%\n",
      "----------------------------------------\n",
      "\n",
      "Best overall model: ahbs_LGB1 with IVRMSE = 0.014952\n",
      "Improvement over baseline: 88.85%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_ivrmse(predictions_df, error_types=['bs', 'ahbs', 'cw'], models=['LGB1', 'LGB2']):\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate IVRMSE for each error type and model\n",
    "    for error_type in error_types:\n",
    "        orig_col = f'iv_{error_type}'\n",
    "        \n",
    "        # Calculate base IVRMSE (before correction)\n",
    "        base_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[orig_col]))\n",
    "        results[f\"{error_type}_base\"] = base_rmse\n",
    "        \n",
    "        # Calculate IVRMSE for each model\n",
    "        for model in models:\n",
    "            corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "            \n",
    "            # Skip if corrected column doesn't exist\n",
    "            if corrected_col not in predictions_df.columns:\n",
    "                print(f\"Warning: {corrected_col} column not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate IVRMSE for the corrected predictions\n",
    "            corrected_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[corrected_col]))\n",
    "            results[f\"{error_type}_{model}\"] = corrected_rmse\n",
    "            \n",
    "            # Calculate improvement percentage\n",
    "            improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "            results[f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the predictions DataFrame (assuming it was saved in the main script)\n",
    "    test_data_with_all_predictions = pd.read_csv('test_data_with_all_predictions_sameday.csv')\n",
    "    \n",
    "    # Calculate IVRMSE\n",
    "    ivrmse_results = calculate_ivrmse(test_data_with_all_predictions)\n",
    "    \n",
    "    # Print results in a table format\n",
    "    print(\"\\n=== IVRMSE Results ===\")\n",
    "    print(f\"{'Model':<15} {'IVRMSE':<10} {'Improvement':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        base_key = f\"{error_type}_base\"\n",
    "        if base_key in ivrmse_results:\n",
    "            base_rmse = ivrmse_results[base_key]\n",
    "            print(f\"{error_type.upper():<15} {base_rmse:.6f}  {'(baseline)':<12}\")\n",
    "            \n",
    "            for model in ['LGB1', 'LGB2']:\n",
    "                model_key = f\"{error_type}_{model}\"\n",
    "                imp_key = f\"{error_type}_{model}_improvement\"\n",
    "                \n",
    "                if model_key in ivrmse_results:\n",
    "                    print(f\"{model_key:<15} {ivrmse_results[model_key]:.6f}  {ivrmse_results[imp_key]:.2f}%\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    # Find best overall model\n",
    "    model_keys = [k for k in ivrmse_results.keys() if not k.endswith('base') and not k.endswith('improvement')]\n",
    "    if model_keys:\n",
    "        best_model = min(model_keys, key=lambda k: ivrmse_results[k])\n",
    "        print(f\"\\nBest overall model: {best_model} with IVRMSE = {ivrmse_results[best_model]:.6f}\")\n",
    "        \n",
    "        base_key = f\"{best_model.split('_')[0]}_base\"\n",
    "        imp_key = f\"{best_model}_improvement\"\n",
    "        if base_key in ivrmse_results and imp_key in ivrmse_results:\n",
    "            print(f\"Improvement over baseline: {ivrmse_results[imp_key]:.2f}%\")\n",
    "            \n",
    "    # Export results to CSV for further analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(ivrmse_results.keys()),\n",
    "        'Value': list(ivrmse_results.values())\n",
    "    })\n",
    "    results_df.to_csv('ivrmse_results_lightgbm_sameday.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05488a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IVRMSE by Moneyness Group (Sameday)\n",
      "================================================================================\n",
      "Moneyness Group  BS Base  BS LGB1  BS LGB2  AHBS Base  AHBS LGB1  AHBS LGB2  CW Base  CW LGB1  CW LGB2\n",
      "          DOTMC 0.135785 0.017140 0.026631   0.135335   0.016947   0.026068 0.122504 0.018262 0.024891\n",
      "           OTMC 0.134271 0.012961 0.021242   0.126841   0.013110   0.020663 0.125176 0.014471 0.021329\n",
      "            ATM 0.140269 0.013238 0.022637   0.135411   0.013368   0.022475 0.131622 0.014085 0.023905\n",
      "           OTMP 0.131156 0.012939 0.021602   0.129842   0.012750   0.022723 0.128338 0.013326 0.022951\n",
      "          DOTMP 0.195506 0.018446 0.030369   0.142425   0.018106   0.029034 0.134354 0.018785 0.028941\n",
      "\n",
      "================================================================================\n",
      "Improvement Percentage by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS LGB1  BS LGB2  AHBS LGB1  AHBS LGB2  CW LGB1  CW LGB2\n",
      "          DOTMC   87.38%   80.39%     87.48%     80.74%   85.09%   79.68%\n",
      "           OTMC   90.35%   84.18%     89.66%     83.71%   88.44%   82.96%\n",
      "            ATM   90.56%   83.86%     90.13%     83.40%   89.30%   81.84%\n",
      "           OTMP   90.13%   83.53%     90.18%     82.50%   89.62%   82.12%\n",
      "          DOTMP   90.57%   84.47%     87.29%     79.61%   86.02%   78.46%\n",
      "\n",
      "================================================================================\n",
      "Best Model by Moneyness Group and Error Type\n",
      "================================================================================\n",
      "Moneyness Group Error Type Best Model Base IVRMSE Best IVRMSE Improvement % Other Model  Other IVRMSE  Other Improvement %\n",
      "          DOTMC         BS       LGB1    0.135785    0.017140        87.38%        LGB2      0.026631            80.387611\n",
      "          DOTMC       AHBS       LGB1    0.135335    0.016947        87.48%        LGB2      0.026068            80.737833\n",
      "          DOTMC         CW       LGB1    0.122504    0.018262        85.09%        LGB2      0.024891            79.681844\n",
      "           OTMC         BS       LGB1    0.134271    0.012961        90.35%        LGB2      0.021242            84.180076\n",
      "           OTMC       AHBS       LGB1    0.126841    0.013110        89.66%        LGB2      0.020663            83.709548\n",
      "           OTMC         CW       LGB1    0.125176    0.014471        88.44%        LGB2      0.021329            82.960759\n",
      "            ATM         BS       LGB1    0.140269    0.013238        90.56%        LGB2      0.022637            83.861900\n",
      "            ATM       AHBS       LGB1    0.135411    0.013368        90.13%        LGB2      0.022475            83.402176\n",
      "            ATM         CW       LGB1    0.131622    0.014085        89.30%        LGB2      0.023905            81.837937\n",
      "           OTMP         BS       LGB1    0.131156    0.012939        90.13%        LGB2      0.021602            83.529320\n",
      "           OTMP       AHBS       LGB1    0.129842    0.012750        90.18%        LGB2      0.022723            82.499126\n",
      "           OTMP         CW       LGB1    0.128338    0.013326        89.62%        LGB2      0.022951            82.117165\n",
      "          DOTMP         BS       LGB1    0.195506    0.018446        90.57%        LGB2      0.030369            84.466246\n",
      "          DOTMP       AHBS       LGB1    0.142425    0.018106        87.29%        LGB2      0.029034            79.614358\n",
      "          DOTMP         CW       LGB1    0.134354    0.018785        86.02%        LGB2      0.028941            78.459225\n",
      "\n",
      "================================================================================\n",
      "Summary Statistics\n",
      "================================================================================\n",
      "Overall best model distribution: {'LGB1': np.int64(15)}\n",
      "\n",
      "Average improvement by moneyness group:\n",
      "  ATM: 90.00%\n",
      "  OTMP: 89.98%\n",
      "  OTMC: 89.48%\n",
      "  DOTMP: 87.96%\n",
      "  DOTMC: 86.65%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the moneyness groups\n",
    "moneyness_groups = ['DOTMC', 'OTMC', 'ATM', 'OTMP', 'DOTMP']\n",
    "\n",
    "def analyze_ivrmse_by_moneyness(test_data):\n",
    "\n",
    "    # 1. Group the test data by moneyness\n",
    "    results = {}\n",
    "    for group in moneyness_groups:\n",
    "        group_data = test_data[test_data['moneyness_category'] == group]\n",
    "        \n",
    "        # Skip if no data in this group\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"Warning: No data found for moneyness group {group}\")\n",
    "            continue\n",
    "            \n",
    "        # Initialize results for this group\n",
    "        results[group] = {}\n",
    "        \n",
    "        # 2. Calculate IVRMSE for each error type and model\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Column names\n",
    "            orig_col = f'iv_{error_type}'\n",
    "            \n",
    "            # Skip if column doesn't exist\n",
    "            if orig_col not in group_data.columns:\n",
    "                print(f\"Warning: Column {orig_col} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate baseline IVRMSE\n",
    "            base_rmse = np.sqrt(mean_squared_error(\n",
    "                group_data['impl_volatility'], \n",
    "                group_data[orig_col]\n",
    "            ))\n",
    "            \n",
    "            # Store baseline IVRMSE\n",
    "            results[group][f\"{error_type}_base\"] = base_rmse\n",
    "            \n",
    "            # Calculate corrected IVRMSE for each model\n",
    "            for model in ['LGB1', 'LGB2']:\n",
    "                corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "                \n",
    "                # Skip if column doesn't exist\n",
    "                if corrected_col not in group_data.columns:\n",
    "                    print(f\"Warning: Column {corrected_col} not found, skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                corrected_rmse = np.sqrt(mean_squared_error(\n",
    "                    group_data['impl_volatility'], \n",
    "                    group_data[corrected_col]\n",
    "                ))\n",
    "                \n",
    "                # Calculate improvement percentage\n",
    "                improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "                \n",
    "                # Store results\n",
    "                results[group][f\"{error_type}_{model}\"] = corrected_rmse\n",
    "                results[group][f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_results_table(results):\n",
    "\n",
    "    # Prepare IVRMSE table data\n",
    "    ivrmse_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if not key.endswith('improvement'):\n",
    "                if key.endswith('base'):\n",
    "                    # Format baseline columns\n",
    "                    error_type = key.split('_')[0].upper()\n",
    "                    row[f\"{error_type} Base\"] = value\n",
    "                else:\n",
    "                    # Format model columns\n",
    "                    parts = key.split('_')\n",
    "                    error_type = parts[0].upper()\n",
    "                    model = parts[1]\n",
    "                    row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        ivrmse_data.append(row)\n",
    "    \n",
    "    # Prepare improvement percentage table data\n",
    "    improvement_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if key.endswith('improvement'):\n",
    "                # Format improvement columns\n",
    "                parts = key.replace('_improvement', '').split('_')\n",
    "                error_type = parts[0].upper()\n",
    "                model = parts[1]\n",
    "                row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        improvement_data.append(row)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    ivrmse_df = pd.DataFrame(ivrmse_data)\n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Sort columns for better readability\n",
    "    ivrmse_cols = ['Moneyness Group']\n",
    "    improvement_cols = ['Moneyness Group']\n",
    "    \n",
    "    for et in ['BS', 'AHBS', 'CW']:\n",
    "        ivrmse_cols.extend([f\"{et} Base\", f\"{et} LGB1\", f\"{et} LGB2\"])\n",
    "        improvement_cols.extend([f\"{et} LGB1\", f\"{et} LGB2\"])\n",
    "    \n",
    "    # Reorder columns if they exist\n",
    "    ivrmse_df = ivrmse_df[[col for col in ivrmse_cols if col in ivrmse_df.columns]]\n",
    "    improvement_df = improvement_df[[col for col in improvement_cols if col in improvement_df.columns]]\n",
    "    \n",
    "    return ivrmse_df, improvement_df\n",
    "\n",
    "def find_best_models(results):\n",
    "\n",
    "    best_models = []\n",
    "    \n",
    "    for group, group_results in results.items():\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Get baseline IVRMSE\n",
    "            base_key = f\"{error_type}_base\"\n",
    "            if base_key not in group_results:\n",
    "                continue\n",
    "                \n",
    "            base_rmse = group_results[base_key]\n",
    "            \n",
    "            # Find best model for this error type\n",
    "            models = [m for m in ['LGB1', 'LGB2'] if f\"{error_type}_{m}\" in group_results]\n",
    "            if not models:\n",
    "                continue\n",
    "            \n",
    "            # Find model with lowest IVRMSE    \n",
    "            best_model = min(models, key=lambda m: group_results[f\"{error_type}_{m}\"])\n",
    "            best_rmse = group_results[f\"{error_type}_{best_model}\"]\n",
    "            improvement = group_results[f\"{error_type}_{best_model}_improvement\"]\n",
    "            \n",
    "            # Also get the values for the other model for comparison\n",
    "            other_models = [m for m in models if m != best_model]\n",
    "            other_model_data = {}\n",
    "            if other_models:\n",
    "                other_model = other_models[0]\n",
    "                other_rmse = group_results[f\"{error_type}_{other_model}\"]\n",
    "                other_improvement = group_results[f\"{error_type}_{other_model}_improvement\"]\n",
    "                other_model_data = {\n",
    "                    f'Other Model': other_model,\n",
    "                    f'Other IVRMSE': other_rmse,\n",
    "                    f'Other Improvement %': other_improvement\n",
    "                }\n",
    "            \n",
    "            model_data = {\n",
    "                'Moneyness Group': group,\n",
    "                'Error Type': error_type.upper(),\n",
    "                'Best Model': best_model,\n",
    "                'Base IVRMSE': base_rmse,\n",
    "                'Best IVRMSE': best_rmse,\n",
    "                'Improvement %': improvement\n",
    "            }\n",
    "            \n",
    "            # Add other model data if available\n",
    "            model_data.update(other_model_data)\n",
    "            \n",
    "            best_models.append(model_data)\n",
    "    \n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "def print_formatted_tables(results):\n",
    "\n",
    "    # Format results into DataFrames\n",
    "    ivrmse_df, improvement_df = format_results_table(results)\n",
    "    \n",
    "    # Format IVRMSE table\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"IVRMSE by Moneyness Group (Sameday)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ivrmse_df.to_string(index=False))\n",
    "    \n",
    "    # Format improvement table\n",
    "    pd.set_option('display.float_format', '{:.2f}%'.format)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Improvement Percentage by Moneyness Group\")\n",
    "    print(\"=\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    best_models_df = find_best_models(results)\n",
    "    \n",
    "    # Reset float format for mixed table\n",
    "    pd.set_option('display.float_format', None)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Best Model by Moneyness Group and Error Type\")\n",
    "    print(\"=\" * 80)\n",
    "    # Format specific columns\n",
    "    best_models_df['Base IVRMSE'] = best_models_df['Base IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Best IVRMSE'] = best_models_df['Best IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Improvement %'] = best_models_df['Improvement %'].map('{:.2f}%'.format)\n",
    "    print(best_models_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Summary Statistics\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count best model occurrences\n",
    "    model_counts = best_models_df['Best Model'].value_counts()\n",
    "    print(f\"Overall best model distribution: {dict(model_counts)}\")\n",
    "    \n",
    "    # Average improvement by moneyness group\n",
    "    print(\"\\nAverage improvement by moneyness group:\")\n",
    "    # Convert percentage strings to numeric values\n",
    "    best_models_df['Improvement_Numeric'] = pd.to_numeric(best_models_df['Improvement %'].str.rstrip('%'))\n",
    "    \n",
    "    # Group and calculate means\n",
    "    avg_improvement = best_models_df.groupby('Moneyness Group')['Improvement_Numeric'].mean()\n",
    "    \n",
    "    # Handle the case where there's only one group (which returns a scalar)\n",
    "    if isinstance(avg_improvement, pd.Series):\n",
    "        # Sort if it's a Series with multiple values\n",
    "        sorted_improvements = avg_improvement.sort_values(ascending=False)\n",
    "        for group, imp in sorted_improvements.items():\n",
    "            print(f\"  {group}: {imp:.2f}%\")\n",
    "    else:\n",
    "        # Just print the single value if it's a scalar\n",
    "        group = best_models_df['Moneyness Group'].iloc[0]\n",
    "        print(f\"  {group}: {avg_improvement:.2f}%\")\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    ivrmse_df.to_csv('ivrmse_by_moneyness_lightgbm.csv', index=False)\n",
    "    improvement_df.to_csv('improvement_by_moneyness_lightgbm.csv', index=False)\n",
    "    best_models_df.to_csv('best_models_by_moneyness_lightgbm.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load test data with predictions\n",
    "    try:\n",
    "        test_data_with_all_predictions = pd.read_csv('test_data_with_all_predictions_sameday.csv')\n",
    "        \n",
    "        # Check if 'moneyness_category' exists, if not, create it\n",
    "        if 'moneyness_category' not in test_data_with_all_predictions.columns:\n",
    "            print(\"Creating moneyness categories...\")\n",
    "            # Define moneyness boundaries\n",
    "            test_data_with_all_predictions['moneyness_category'] = pd.cut(\n",
    "                test_data_with_all_predictions['moneyness'],\n",
    "                bins=[-float('inf'), 0.9, 0.97, 1.03, 1.1, float('inf')],\n",
    "                labels=moneyness_groups\n",
    "            )\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyze_ivrmse_by_moneyness(test_data_with_all_predictions)\n",
    "        print_formatted_tables(results)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Test data file not found. Please run the main script first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
