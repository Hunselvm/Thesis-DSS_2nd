{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e9b7fe",
   "metadata": {},
   "source": [
    "Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   impl_volatility  cp_flag  stock_price  moneyness  time_to_expiry  \\\n",
      "0         0.210270        1      115.545   0.970966              24   \n",
      "1         0.208124        1      115.545   0.962875              24   \n",
      "2         0.205474        1      115.545   0.954917              24   \n",
      "3         0.278442        0      115.545   1.100429              24   \n",
      "4         0.242212        0      115.545   1.050409              24   \n",
      "\n",
      "   strike_price     delta     gamma       vega      theta      rf   iv_ahbs  \\\n",
      "0         119.0  0.329402  0.057909  10.772000 -16.612410  0.0245  0.346996   \n",
      "1         120.0  0.273000  0.053728   9.892975 -15.153390  0.0245  0.346099   \n",
      "2         121.0  0.220664  0.048533   8.819048 -13.381580  0.0245  0.345399   \n",
      "3         105.0 -0.075725  0.017194   4.246497  -9.138635  0.0245  0.386757   \n",
      "4         110.0 -0.186759  0.037192   7.995862 -15.079300  0.0245  0.365726   \n",
      "\n",
      "   iv_ahbs_error     iv_bs  iv_bs_error     iv_cw  iv_cw_error  \n",
      "0       0.136726  0.353798     0.143528  0.316948     0.106678  \n",
      "1       0.137975  0.353798     0.145674  0.318967     0.110843  \n",
      "2       0.139925  0.353798     0.148324  0.321854     0.116380  \n",
      "3       0.108315  0.353798     0.075356  0.388674     0.110232  \n",
      "4       0.123514  0.353798     0.111586  0.342023     0.099811  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance as sklearn_permutation_importance\n",
    "   \n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_train_sameday_real.csv')\n",
    "test_data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_test_sameday_real.csv')\n",
    "columns_to_remove = ['volume', 'option_price', 'open_interest']\n",
    "test_data = test_data.drop(columns=columns_to_remove)\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "test_data_with_all_predictions = test_data.copy() \n",
    "\n",
    "# Make a copy for predictions\n",
    "test_data_with_all_predictions = test_data.copy()\n",
    "\n",
    "option_columns = [\n",
    "    'impl_volatility',\n",
    "    'cp_flag',\n",
    "    'stock_price',\n",
    "    'moneyness',\n",
    "    'time_to_expiry',\n",
    "    'strike_price',\n",
    "    'delta',\n",
    "    'gamma',\n",
    "    'vega',\n",
    "    'theta',\n",
    "    'rf',\n",
    "    'iv_ahbs',\n",
    "    'iv_ahbs_error',\n",
    "    'iv_bs',\n",
    "    'iv_bs_error',  # Adjusted from a duplicate iv_ahbs_error\n",
    "    'iv_cw',\n",
    "    'iv_cw_error'\n",
    "]\n",
    "\n",
    "option_only = data[option_columns]\n",
    "test_data_with_all_predictions = test_data_with_all_predictions[option_columns]\n",
    "print(option_only.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d038c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>stock_price</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>time_to_expiry</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>delta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>vega</th>\n",
       "      <th>theta</th>\n",
       "      <th>...</th>\n",
       "      <th>iv_bs_pred_NN4</th>\n",
       "      <th>iv_bs_corrected_NN4</th>\n",
       "      <th>iv_ahbs_pred_NN3</th>\n",
       "      <th>iv_ahbs_corrected_NN3</th>\n",
       "      <th>iv_ahbs_pred_NN4</th>\n",
       "      <th>iv_ahbs_corrected_NN4</th>\n",
       "      <th>iv_cw_pred_NN3</th>\n",
       "      <th>iv_cw_corrected_NN3</th>\n",
       "      <th>iv_cw_pred_NN4</th>\n",
       "      <th>iv_cw_corrected_NN4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "      <td>1.089824e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.537372e-01</td>\n",
       "      <td>5.384209e-01</td>\n",
       "      <td>2.168925e+02</td>\n",
       "      <td>1.020033e+00</td>\n",
       "      <td>7.300243e+01</td>\n",
       "      <td>2.147561e+02</td>\n",
       "      <td>5.406997e-02</td>\n",
       "      <td>3.874991e-02</td>\n",
       "      <td>2.547875e+01</td>\n",
       "      <td>-3.387311e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.615054e-03</td>\n",
       "      <td>3.564132e-01</td>\n",
       "      <td>-2.164857e-03</td>\n",
       "      <td>3.558459e-01</td>\n",
       "      <td>-1.970897e-03</td>\n",
       "      <td>3.556519e-01</td>\n",
       "      <td>-2.434737e-03</td>\n",
       "      <td>3.559069e-01</td>\n",
       "      <td>-3.603944e-03</td>\n",
       "      <td>3.570761e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.505677e-01</td>\n",
       "      <td>4.985219e-01</td>\n",
       "      <td>4.091524e+02</td>\n",
       "      <td>1.356553e-01</td>\n",
       "      <td>5.502698e+01</td>\n",
       "      <td>4.113761e+02</td>\n",
       "      <td>3.056829e-01</td>\n",
       "      <td>5.054576e-02</td>\n",
       "      <td>5.188064e+01</td>\n",
       "      <td>7.601737e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.473609e-01</td>\n",
       "      <td>1.473609e-01</td>\n",
       "      <td>1.304187e-01</td>\n",
       "      <td>1.472153e-01</td>\n",
       "      <td>1.304020e-01</td>\n",
       "      <td>1.470581e-01</td>\n",
       "      <td>1.249892e-01</td>\n",
       "      <td>1.472917e-01</td>\n",
       "      <td>1.248199e-01</td>\n",
       "      <td>1.470375e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.641200e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.240000e+00</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>-9.152470e-01</td>\n",
       "      <td>6.000000e-06</td>\n",
       "      <td>1.042100e-02</td>\n",
       "      <td>-1.691377e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110648e+00</td>\n",
       "      <td>7.020296e-02</td>\n",
       "      <td>-2.004421e+00</td>\n",
       "      <td>1.823548e-02</td>\n",
       "      <td>-2.017903e+00</td>\n",
       "      <td>7.051928e-02</td>\n",
       "      <td>-1.853428e+00</td>\n",
       "      <td>8.636025e-02</td>\n",
       "      <td>-1.885655e+00</td>\n",
       "      <td>6.258065e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.453760e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.413500e+01</td>\n",
       "      <td>9.312373e-01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>5.500000e+01</td>\n",
       "      <td>-2.140933e-01</td>\n",
       "      <td>8.446000e-03</td>\n",
       "      <td>5.972642e+00</td>\n",
       "      <td>-2.855597e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.802840e-02</td>\n",
       "      <td>2.501075e-01</td>\n",
       "      <td>-6.741001e-02</td>\n",
       "      <td>2.496532e-01</td>\n",
       "      <td>-6.711460e-02</td>\n",
       "      <td>2.497281e-01</td>\n",
       "      <td>-6.210580e-02</td>\n",
       "      <td>2.496547e-01</td>\n",
       "      <td>-6.340673e-02</td>\n",
       "      <td>2.510063e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.201395e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.165650e+02</td>\n",
       "      <td>9.918776e-01</td>\n",
       "      <td>5.100000e+01</td>\n",
       "      <td>1.150000e+02</td>\n",
       "      <td>9.269950e-02</td>\n",
       "      <td>2.093600e-02</td>\n",
       "      <td>1.196315e+01</td>\n",
       "      <td>-1.209609e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.983008e-02</td>\n",
       "      <td>3.239680e-01</td>\n",
       "      <td>2.455792e-02</td>\n",
       "      <td>3.234262e-01</td>\n",
       "      <td>2.487505e-02</td>\n",
       "      <td>3.232279e-01</td>\n",
       "      <td>2.649692e-02</td>\n",
       "      <td>3.233944e-01</td>\n",
       "      <td>2.531405e-02</td>\n",
       "      <td>3.245431e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.289585e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.024950e+02</td>\n",
       "      <td>1.080526e+00</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>3.259722e-01</td>\n",
       "      <td>4.957800e-02</td>\n",
       "      <td>2.491818e+01</td>\n",
       "      <td>-5.561454e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036906e-01</td>\n",
       "      <td>4.318265e-01</td>\n",
       "      <td>8.839633e-02</td>\n",
       "      <td>4.311001e-01</td>\n",
       "      <td>8.839913e-02</td>\n",
       "      <td>4.309427e-01</td>\n",
       "      <td>8.502727e-02</td>\n",
       "      <td>4.312781e-01</td>\n",
       "      <td>8.371254e-02</td>\n",
       "      <td>4.324269e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.712715e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.727041e+03</td>\n",
       "      <td>1.600000e+00</td>\n",
       "      <td>2.400000e+02</td>\n",
       "      <td>4.600000e+03</td>\n",
       "      <td>7.801590e-01</td>\n",
       "      <td>1.321314e+00</td>\n",
       "      <td>1.139503e+03</td>\n",
       "      <td>-2.919800e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.835951e-01</td>\n",
       "      <td>2.464446e+00</td>\n",
       "      <td>4.517831e-01</td>\n",
       "      <td>2.562475e+00</td>\n",
       "      <td>4.044157e-01</td>\n",
       "      <td>2.552457e+00</td>\n",
       "      <td>2.906757e-01</td>\n",
       "      <td>2.527477e+00</td>\n",
       "      <td>3.406985e-01</td>\n",
       "      <td>2.494889e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       impl_volatility       cp_flag   stock_price     moneyness  \\\n",
       "count     1.089824e+06  1.089824e+06  1.089824e+06  1.089824e+06   \n",
       "mean      3.537372e-01  5.384209e-01  2.168925e+02  1.020033e+00   \n",
       "std       1.505677e-01  4.985219e-01  4.091524e+02  1.356553e-01   \n",
       "min       2.641200e-02  0.000000e+00  4.240000e+00  8.000000e-01   \n",
       "25%       2.453760e-01  0.000000e+00  5.413500e+01  9.312373e-01   \n",
       "50%       3.201395e-01  1.000000e+00  1.165650e+02  9.918776e-01   \n",
       "75%       4.289585e-01  1.000000e+00  2.024950e+02  1.080526e+00   \n",
       "max       2.712715e+00  1.000000e+00  3.727041e+03  1.600000e+00   \n",
       "\n",
       "       time_to_expiry  strike_price         delta         gamma          vega  \\\n",
       "count    1.089824e+06  1.089824e+06  1.089824e+06  1.089824e+06  1.089824e+06   \n",
       "mean     7.300243e+01  2.147561e+02  5.406997e-02  3.874991e-02  2.547875e+01   \n",
       "std      5.502698e+01  4.113761e+02  3.056829e-01  5.054576e-02  5.188064e+01   \n",
       "min      2.000000e+01  3.000000e+00 -9.152470e-01  6.000000e-06  1.042100e-02   \n",
       "25%      3.100000e+01  5.500000e+01 -2.140933e-01  8.446000e-03  5.972642e+00   \n",
       "50%      5.100000e+01  1.150000e+02  9.269950e-02  2.093600e-02  1.196315e+01   \n",
       "75%      1.000000e+02  2.000000e+02  3.259722e-01  4.957800e-02  2.491818e+01   \n",
       "max      2.400000e+02  4.600000e+03  7.801590e-01  1.321314e+00  1.139503e+03   \n",
       "\n",
       "              theta  ...  iv_bs_pred_NN4  iv_bs_corrected_NN4  \\\n",
       "count  1.089824e+06  ...    1.089824e+06         1.089824e+06   \n",
       "mean  -3.387311e+01  ...   -2.615054e-03         3.564132e-01   \n",
       "std    7.601737e+01  ...    1.473609e-01         1.473609e-01   \n",
       "min   -1.691377e+03  ...   -2.110648e+00         7.020296e-02   \n",
       "25%   -2.855597e+01  ...   -7.802840e-02         2.501075e-01   \n",
       "50%   -1.209609e+01  ...    2.983008e-02         3.239680e-01   \n",
       "75%   -5.561454e+00  ...    1.036906e-01         4.318265e-01   \n",
       "max   -2.919800e-02  ...    2.835951e-01         2.464446e+00   \n",
       "\n",
       "       iv_ahbs_pred_NN3  iv_ahbs_corrected_NN3  iv_ahbs_pred_NN4  \\\n",
       "count      1.089824e+06           1.089824e+06      1.089824e+06   \n",
       "mean      -2.164857e-03           3.558459e-01     -1.970897e-03   \n",
       "std        1.304187e-01           1.472153e-01      1.304020e-01   \n",
       "min       -2.004421e+00           1.823548e-02     -2.017903e+00   \n",
       "25%       -6.741001e-02           2.496532e-01     -6.711460e-02   \n",
       "50%        2.455792e-02           3.234262e-01      2.487505e-02   \n",
       "75%        8.839633e-02           4.311001e-01      8.839913e-02   \n",
       "max        4.517831e-01           2.562475e+00      4.044157e-01   \n",
       "\n",
       "       iv_ahbs_corrected_NN4  iv_cw_pred_NN3  iv_cw_corrected_NN3  \\\n",
       "count           1.089824e+06    1.089824e+06         1.089824e+06   \n",
       "mean            3.556519e-01   -2.434737e-03         3.559069e-01   \n",
       "std             1.470581e-01    1.249892e-01         1.472917e-01   \n",
       "min             7.051928e-02   -1.853428e+00         8.636025e-02   \n",
       "25%             2.497281e-01   -6.210580e-02         2.496547e-01   \n",
       "50%             3.232279e-01    2.649692e-02         3.233944e-01   \n",
       "75%             4.309427e-01    8.502727e-02         4.312781e-01   \n",
       "max             2.552457e+00    2.906757e-01         2.527477e+00   \n",
       "\n",
       "       iv_cw_pred_NN4  iv_cw_corrected_NN4  \n",
       "count    1.089824e+06         1.089824e+06  \n",
       "mean    -3.603944e-03         3.570761e-01  \n",
       "std      1.248199e-01         1.470375e-01  \n",
       "min     -1.885655e+00         6.258065e-02  \n",
       "25%     -6.340673e-02         2.510063e-01  \n",
       "50%      2.531405e-02         3.245431e-01  \n",
       "75%      8.371254e-02         4.324269e-01  \n",
       "max      3.406985e-01         2.494889e+00  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_with_all_predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 1: NEURAL NETWORK MODEL DEFINITION\n",
    "###########################################\n",
    "\n",
    "def create_nn_model(architecture_type, input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define architecture based on specifications\n",
    "    if architecture_type == 'NN3':\n",
    "        # Three hidden layers with 32, 16, and 8 neurons\n",
    "        model.add(Dense(128, activation='sigmoid', input_dim=input_dim))\n",
    "        model.add(Dense(64, activation='sigmoid'))\n",
    "        model.add(Dense(32, activation='sigmoid'))\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    elif architecture_type == 'NN4':\n",
    "        # Four hidden layers with 32, 16, 8, and 4 neurons\n",
    "        model.add(Dense(128, activation='sigmoid', input_dim=input_dim))\n",
    "        model.add(Dense(64, activation='sigmoid'))\n",
    "        model.add(Dense(32, activation='sigmoid'))\n",
    "        model.add(Dense(16, activation='sigmoid'))\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid architecture type. Choose from 'NN3' or 'NN4'.\")\n",
    "    \n",
    "    # Compile model with Adam optimizer for faster training\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, epochs=100):\n",
    "\n",
    "    # Early stopping with optimized parameters\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-5,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model with larger batch size for better speed\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_split=0.0,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test_scaled, batch_size=128, verbose=0)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return history, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66054835",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 2: DATA PREPARATION\n",
    "###########################################\n",
    "\n",
    "def prepare_data(option_only):\n",
    "\n",
    "    # Prepare features and target variables\n",
    "    feature_columns = [col for col in option_only.columns if col not in \n",
    "                      ['iv_bs_error', 'iv_ahbs', \"iv_ahbs_error\", \"iv_bs\", \n",
    "                       \"iv_cw\", \"iv_cw_error\", \"impl_volatility\", \"moneyness_category\", 'ID']]\n",
    "    \n",
    "    # Instead of using train_test_split, use your predefined sets\n",
    "    X_train = option_only[feature_columns]  # Features from your training set\n",
    "    X_test = test_data[feature_columns]  # Features from your test set\n",
    "\n",
    "    # Target variables for each error type\n",
    "    y_bs_train = option_only['iv_bs_error']  \n",
    "    y_bs_test = test_data['iv_bs_error']  \n",
    "\n",
    "    y_ahbs_train = option_only['iv_ahbs_error']  \n",
    "    y_ahbs_test = test_data['iv_ahbs_error'] \n",
    "\n",
    "    y_cw_train = option_only['iv_cw_error']  \n",
    "    y_cw_test = test_data['iv_cw_error'] \n",
    "    # Initialize a StandardScaler to normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on the training data and transform both training and testing data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create dictionaries to store multiple target variables\n",
    "    y_train_dict = {\n",
    "        'bs': y_bs_train,\n",
    "        'ahbs': y_ahbs_train,\n",
    "        'cw': y_cw_train\n",
    "    }\n",
    "    \n",
    "    y_test_dict = {\n",
    "        'bs': y_bs_test,\n",
    "        'ahbs': y_ahbs_test,\n",
    "        'cw': y_cw_test\n",
    "    }\n",
    "\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    joblib.dump(feature_columns, 'feature_columns.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_dict, y_test_dict, scaler, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a205ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_add_to_test_data(models, test_data, feature_columns, scaler, error_type):\n",
    "\n",
    "    # Create a copy of the test data to avoid modifying the original\n",
    "    result_df = test_data\n",
    "   \n",
    "    # Extract features from test data as DataFrame (not as numpy array)\n",
    "    X_test = test_data[feature_columns]\n",
    "   \n",
    "    # Scale the features using the pre-fitted scaler and convert back to DataFrame\n",
    "    # This preserves feature names and prevents the warning\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=feature_columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "   \n",
    "    # Original value column name\n",
    "    original_column = f'iv_{error_type}'\n",
    "   \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test_scaled, batch_size=128, verbose=0)\n",
    "       \n",
    "        # Flatten predictions if needed\n",
    "        if len(predictions.shape) > 1:\n",
    "            predictions = predictions.flatten()\n",
    "       \n",
    "        # Add predictions to the dataframe\n",
    "        column_name = f'iv_{error_type}_pred_{model_name}'\n",
    "        result_df[column_name] = predictions\n",
    "       \n",
    "        # Calculate corrected value by adding the error prediction to the original value\n",
    "        result_df[f'iv_{error_type}_corrected_{model_name}'] = result_df[original_column] - predictions\n",
    "   \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb6278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n=== Training models for bs error correction ===\n",
      "/nTraining NN3_bs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 616us/step - loss: 0.0041 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 603us/step - loss: 4.2579e-04 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 602us/step - loss: 3.6727e-04 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 622us/step - loss: 2.8614e-04 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 656us/step - loss: 2.7854e-04 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 609us/step - loss: 2.4248e-04 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 596us/step - loss: 2.6145e-04 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 605us/step - loss: 2.6389e-04 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 631us/step - loss: 1.9776e-04 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 606us/step - loss: 2.0681e-04 - learning_rate: 0.0100\n",
      "Epoch 11/50\n",
      "\u001b[1m12764/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 2.1116e-04\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 609us/step - loss: 2.1116e-04 - learning_rate: 0.0100\n",
      "Epoch 12/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 628us/step - loss: 1.0787e-04 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 622us/step - loss: 9.9115e-05 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m12772/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 616us/step - loss: 9.4956e-05 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m 9568/12772\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 627us/step - loss: 9.2530e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m model = create_nn_model(nn_type, X_train_scaled.shape[\u001b[32m1\u001b[39m])\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Train and evaluate model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m history, mse = \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Store model and results\u001b[39;00m\n\u001b[32m     47\u001b[39m all_models[error_type][nn_type] = model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mtrain_and_evaluate_model\u001b[39m\u001b[34m(model, X_train_scaled, y_train, X_test_scaled, y_test, epochs)\u001b[39m\n\u001b[32m     59\u001b[39m callbacks = [\n\u001b[32m     60\u001b[39m     EarlyStopping(\n\u001b[32m     61\u001b[39m         monitor=\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     )\n\u001b[32m     73\u001b[39m ]\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Train model with larger batch size for better speed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     83\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[32m     86\u001b[39m y_pred = model.predict(X_test_scaled, batch_size=\u001b[32m128\u001b[39m, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m ):\n\u001b[32m    219\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "# PART 4: COMPLETE WORKFLOW\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Step 2: Prepare data\n",
    "    X_train_scaled, X_test_scaled, y_train_dict, y_test_dict, scaler, feature_columns = prepare_data(option_only)\n",
    "    \n",
    "    # Step 3: Train models for each target variable\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Dictionary to store all models\n",
    "    all_models = {\n",
    "        'bs': {},\n",
    "        'ahbs': {},\n",
    "        'cw': {}\n",
    "    }\n",
    "    \n",
    "    # Train models for each error type\n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        print(f\"/n=== Training models for {error_type} error correction ===\")\n",
    "        \n",
    "        # Get the appropriate training and test targets\n",
    "        y_train = y_train_dict[error_type]\n",
    "        y_test = y_test_dict[error_type]\n",
    "        \n",
    "        # Train each architecture\n",
    "        for nn_type in ['NN3', 'NN4']:\n",
    "            model_name = f\"{nn_type}_{error_type}\"\n",
    "            print(f\"/nTraining {model_name}...\")\n",
    "            \n",
    "            # Create model\n",
    "            model = create_nn_model(nn_type, X_train_scaled.shape[1])\n",
    "            \n",
    "            # Train and evaluate model\n",
    "            history, mse = train_and_evaluate_model(\n",
    "                model, X_train_scaled, y_train, X_test_scaled, y_test, epochs=50\n",
    "            )\n",
    "            \n",
    "            # Store model and results\n",
    "            all_models[error_type][nn_type] = model\n",
    "            results[model_name] = {\n",
    "                'mse': mse,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_name} Test MSE: {mse}\")\n",
    "            \n",
    "            # Save model\n",
    "            save_model(model, f\"{model_name}_model_sameday.h5\")\n",
    "    \n",
    "    # Step 4: Generate predictions on new test data for each error type\n",
    "for error_type in ['bs', 'ahbs', 'cw']:\n",
    "    print(f\"\\n=== Making predictions for {error_type} error correction ===\")\n",
    "    \n",
    "    # Extract the models for this error type\n",
    "    current_models = all_models[error_type]\n",
    "    \n",
    "    # Make predictions - use the accumulated DataFrame\n",
    "    test_data_with_all_predictions = predict_and_add_to_test_data(\n",
    "        current_models, test_data_with_all_predictions,feature_columns,  scaler, error_type\n",
    "    )\n",
    "    \n",
    "    # Save intermediate results if desired\n",
    "    test_data_with_all_predictions.to_csv(f'test_data_with_{error_type}_predictions_sameday.csv', index=False)\n",
    "    \n",
    "    # Display sample results from accumulated DataFrame\n",
    "    print(f\"\\nSample of test data with {error_type} predictions:\")\n",
    "    display_columns = ['iv_' + error_type]\n",
    "    for nn_type in ['NN3', 'NN4']:\n",
    "        display_columns.extend([\n",
    "            f'iv_{error_type}_pred_{nn_type}', \n",
    "            f'iv_{error_type}_corrected_{nn_type}'\n",
    "        ])\n",
    "    \n",
    "    print(test_data_with_all_predictions[display_columns].head(5))\n",
    "    save_model(model, f\"{model_name}_model_sameday.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2b159",
   "metadata": {},
   "source": [
    "Calculate IVRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03110ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bs NN3...\n",
      "✓ Model loaded successfully with 4 layers\n",
      "Loading bs NN4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully with 5 layers\n",
      "Loading ahbs NN3...\n",
      "✓ Model loaded successfully with 4 layers\n",
      "Loading ahbs NN4...\n",
      "✓ Model loaded successfully with 5 layers\n",
      "Loading cw NN3...\n",
      "✓ Model loaded successfully with 4 layers\n",
      "Loading cw NN4...\n",
      "✓ Model loaded successfully with 5 layers\n",
      "\n",
      "=== Model Loading Summary ===\n",
      "bs NN3: ✓ Loaded\n",
      "bs NN4: ✓ Loaded\n",
      "ahbs NN3: ✓ Loaded\n",
      "ahbs NN4: ✓ Loaded\n",
      "cw NN3: ✓ Loaded\n",
      "cw NN4: ✓ Loaded\n"
     ]
    }
   ],
   "source": [
    "def load_models(model_paths_dict):\n",
    "\n",
    "    loaded_models = {}\n",
    "    \n",
    "    for error_type in model_paths_dict:\n",
    "        loaded_models[error_type] = {}\n",
    "        \n",
    "        for model_name, path in model_paths_dict[error_type].items():\n",
    "            try:\n",
    "                # Handle Windows paths by using raw strings\n",
    "                path = path.replace('\\\\', '/')  # Convert backslashes to forward slashes\n",
    "                \n",
    "                # Check if file exists\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"✗ {error_type} {model_name}: File not found at {path}\")\n",
    "                    loaded_models[error_type][model_name] = None\n",
    "                    continue\n",
    "                \n",
    "                # Load the model\n",
    "                print(f\"Loading {error_type} {model_name}...\")\n",
    "                model = load_model(path)\n",
    "                \n",
    "                # Simple check: can we access layers?\n",
    "                num_layers = len(model.layers)\n",
    "                print(f\"✓ Model loaded successfully with {num_layers} layers\")\n",
    "                \n",
    "                # Store loaded model\n",
    "                loaded_models[error_type][model_name] = model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load {error_type} {model_name}: {str(e)}\")\n",
    "                loaded_models[error_type][model_name] = None\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model paths dictionary with proper path handling for Windows\n",
    "    # Use forward slashes or raw strings to avoid Unicode escape errors\n",
    "    model_paths_dict = {\n",
    "        'bs': {\n",
    "            'NN3': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network SameDay\\Without characteristics/NN3_bs_model_sameday.h5\",\n",
    "            'NN4': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network SameDay\\Without characteristics/NN4_bs_model_sameday.h5\"\n",
    "        },\n",
    "        'ahbs': {\n",
    "            'NN3': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network SameDay\\Without characteristics/NN3_ahbs_model_sameday.h5\",\n",
    "            'NN4': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network SameDay\\Without characteristics/NN4_ahbs_model_sameday.h5\"\n",
    "        },\n",
    "        'cw': {\n",
    "            'NN3': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network SameDay\\Without characteristics/NN3_cw_model_sameday.h5\",\n",
    "            'NN4': r\"C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Code/Models/Neural Network SameDay\\Without characteristics/NN4_cw_model_sameday.h5\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load all models\n",
    "    loaded_models = load_models(model_paths_dict)\n",
    "    \n",
    "    # Print summary of loaded models\n",
    "    print(\"\\n=== Model Loading Summary ===\")\n",
    "    for error_type in loaded_models:\n",
    "        for model_name in loaded_models[error_type]:\n",
    "            status = \"✓ Loaded\" if loaded_models[error_type][model_name] is not None else \"✗ Failed\"\n",
    "            print(f\"{error_type} {model_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60137c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cp_flag', 'stock_price', 'moneyness', 'time_to_expiry', 'strike_price', 'delta', 'gamma', 'vega', 'theta', 'rf']\n",
      "\n",
      "-- predicting corrections for bs --\n",
      "   impl_volatility     iv_bs  iv_bs_pred_NN3  iv_bs_corrected_NN3  \\\n",
      "0         0.217247  0.353798        0.142212             0.211586   \n",
      "1         0.215223  0.353798        0.144742             0.209056   \n",
      "2         0.212591  0.353798        0.147380             0.206419   \n",
      "\n",
      "   iv_bs_pred_NN4  iv_bs_corrected_NN4  \n",
      "0        0.141132             0.212666  \n",
      "1        0.144622             0.209176  \n",
      "2        0.147485             0.206313  \n",
      "\n",
      "-- predicting corrections for ahbs --\n",
      "   impl_volatility   iv_ahbs  iv_ahbs_pred_NN3  iv_ahbs_corrected_NN3  \\\n",
      "0         0.217247  0.350970          0.135346               0.215624   \n",
      "1         0.215223  0.349422          0.137301               0.212120   \n",
      "2         0.212591  0.348100          0.139177               0.208924   \n",
      "\n",
      "   iv_ahbs_pred_NN4  iv_ahbs_corrected_NN4  \n",
      "0          0.138737               0.212234  \n",
      "1          0.139591               0.209831  \n",
      "2          0.141259               0.206842  \n",
      "\n",
      "-- predicting corrections for cw --\n",
      "   impl_volatility     iv_cw  iv_cw_pred_NN3  iv_cw_corrected_NN3  \\\n",
      "0         0.217247  0.316475        0.107924             0.208551   \n",
      "1         0.215223  0.315673        0.109284             0.206389   \n",
      "2         0.212591  0.315839        0.111485             0.204355   \n",
      "\n",
      "   iv_cw_pred_NN4  iv_cw_corrected_NN4  \n",
      "0        0.102500             0.213975  \n",
      "1        0.104479             0.211194  \n",
      "2        0.107111             0.208729  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler_path = r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\Neural Network SameDay\\Without characteristics\\scaler.pkl\"\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "base_df = test_data_with_all_predictions\n",
    "\n",
    "# 2) determine your features once\n",
    "exclude_cols = [\n",
    "    'iv_bs_error','iv_ahbs','iv_ahbs_error','iv_bs','iv_cw','iv_cw_error',\n",
    "    'impl_volatility','moneyness_category','ID','date','new_id',\n",
    "    'open_interest','option_price','prediction_horizon',\n",
    "    'test_date','train_date','volume', 'iv_bs_pred_NN3', 'iv_bs_corrected_NN3', 'iv_bs_pred_NN4', 'iv_bs_corrected_NN4', 'iv_ahbs_pred_NN3', 'iv_ahbs_corrected_NN3', 'iv_ahbs_pred_NN4', 'iv_ahbs_corrected_NN4', 'iv_cw_pred_NN3', 'iv_cw_corrected_NN3', 'iv_cw_pred_NN4', 'iv_cw_corrected_NN4'\n",
    "]\n",
    "feature_columns = [c for c in base_df.columns if c not in exclude_cols]\n",
    "print(feature_columns)\n",
    "# 3) loop over error types _and_ models, but update the SAME DataFrame\n",
    "df = base_df\n",
    "for error_type in ['bs','ahbs','cw']:\n",
    "    print(f\"\\n-- predicting corrections for {error_type} --\")\n",
    "    \n",
    "    models_for_type = loaded_models[error_type]  # e.g. [nn3_model, nn4_model]\n",
    "    \n",
    "    # assume your helper loops through each model internally,\n",
    "    # adding both iv_{error_type}_pred_NN3/_NN4 and iv_{error_type}_corrected_NN3/_NN4\n",
    "    df = predict_and_add_to_test_data(\n",
    "        models_for_type, df, feature_columns, scaler, error_type\n",
    "    )\n",
    "    \n",
    "    # show a quick peek\n",
    "    cols = ['impl_volatility', f'iv_{error_type}']\n",
    "    for m in ['NN3','NN4']:\n",
    "        cols += [f'iv_{error_type}_pred_{m}', f'iv_{error_type}_corrected_{m}']\n",
    "    print(df[cols].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b089598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IVRMSE Results ===\n",
      "Model           IVRMSE     Improvement \n",
      "----------------------------------------\n",
      "BS              0.150568  (baseline)  \n",
      "bs_NN3          0.011421  92.41%\n",
      "bs_NN4          0.011191  92.57%\n",
      "----------------------------------------\n",
      "AHBS            0.134068  (baseline)  \n",
      "ahbs_NN3        0.011442  91.47%\n",
      "ahbs_NN4        0.011282  91.58%\n",
      "----------------------------------------\n",
      "CW              0.128764  (baseline)  \n",
      "cw_NN3          0.012688  90.15%\n",
      "cw_NN4          0.013123  89.81%\n",
      "----------------------------------------\n",
      "\n",
      "Best overall model: bs_NN4 with IVRMSE = 0.011191\n",
      "Improvement over baseline: 92.57%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calculate_ivrmse(predictions_df, error_types=['bs', 'ahbs', 'cw'], models=['NN3', 'NN4']):\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate IVRMSE for each error type and model\n",
    "    for error_type in error_types:\n",
    "        orig_col = f'iv_{error_type}'\n",
    "        \n",
    "        # Calculate base IVRMSE (before correction)\n",
    "        base_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[orig_col]))\n",
    "        results[f\"{error_type}_base\"] = base_rmse\n",
    "        \n",
    "        # Calculate IVRMSE for each model\n",
    "        for model in models:\n",
    "            corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "            \n",
    "            # Skip if corrected column doesn't exist\n",
    "            if corrected_col not in predictions_df.columns:\n",
    "                print(f\"Warning: {corrected_col} column not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate IVRMSE for the corrected predictions\n",
    "            corrected_diff = predictions_df['impl_volatility'] - predictions_df[corrected_col]\n",
    "            corrected_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[corrected_col]))\n",
    "            results[f\"{error_type}_{model}\"] = corrected_rmse\n",
    "            \n",
    "            # Calculate improvement percentage\n",
    "            improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "            results[f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming test_data_with_all_predictions is your DataFrame with all predictions\n",
    "    ivrmse_results = calculate_ivrmse(test_data_with_all_predictions)\n",
    "    \n",
    "    # Print results in a table format\n",
    "    print(\"\\n=== IVRMSE Results ===\")\n",
    "    print(f\"{'Model':<15} {'IVRMSE':<10} {'Improvement':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        base_key = f\"{error_type}_base\"\n",
    "        if base_key in ivrmse_results:\n",
    "            base_rmse = ivrmse_results[base_key]\n",
    "            print(f\"{error_type.upper():<15} {base_rmse:.6f}  {'(baseline)':<12}\")\n",
    "            \n",
    "            for model in ['NN3', 'NN4']:\n",
    "                model_key = f\"{error_type}_{model}\"\n",
    "                imp_key = f\"{error_type}_{model}_improvement\"\n",
    "                \n",
    "                if model_key in ivrmse_results:\n",
    "                    print(f\"{model_key:<15} {ivrmse_results[model_key]:.6f}  {ivrmse_results[imp_key]:.2f}%\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    # Find best overall model\n",
    "    model_keys = [k for k in ivrmse_results.keys() if not k.endswith('base') and not k.endswith('improvement')]\n",
    "    if model_keys:\n",
    "        best_model = min(model_keys, key=lambda k: ivrmse_results[k])\n",
    "        print(f\"\\nBest overall model: {best_model} with IVRMSE = {ivrmse_results[best_model]:.6f}\")\n",
    "        \n",
    "        base_key = f\"{best_model.split('_')[0]}_base\"\n",
    "        imp_key = f\"{best_model}_improvement\"\n",
    "        if base_key in ivrmse_results and imp_key in ivrmse_results:\n",
    "            print(f\"Improvement over baseline: {ivrmse_results[imp_key]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd168e8",
   "metadata": {},
   "source": [
    "IVRMSE Moneyness categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "355c9edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added moneyness categories to test_data_per_horizon\n",
      "\n",
      "Sample data for horizon:\n",
      "   cp_flag  moneyness moneyness_category\n",
      "0        1   0.996078                ATM\n",
      "1        1   0.987564                ATM\n",
      "2        1   0.979195                ATM\n",
      "3        1   0.924360               OTMC\n",
      "4        0   1.031652               OTMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Temp\\ipykernel_24432\\347831484.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'DOTMC' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[dotmc_mask, 'moneyness_category'] = 'DOTMC'\n"
     ]
    }
   ],
   "source": [
    "def add_moneyness_categories(df):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Initialize moneyness_category column with NaN values\n",
    "    result['moneyness_category'] = np.nan\n",
    "    \n",
    "    # Define conditions for each category\n",
    "    dotmc_mask = (result['cp_flag'] == 1) & (result['moneyness'] >= 0.80) & (result['moneyness'] < 0.90)\n",
    "    otmc_mask = (result['cp_flag'] == 1) & (result['moneyness'] >= 0.90) & (result['moneyness'] < 0.97)\n",
    "    atm_mask = (result['moneyness'] >= 0.97) & (result['moneyness'] < 1.03)\n",
    "    otmp_mask = (result['cp_flag'] == 0) & (result['moneyness'] >= 1.03) & (result['moneyness'] < 1.10)\n",
    "    dotmp_mask = (result['cp_flag'] == 0) & (result['moneyness'] >= 1.10) & (result['moneyness'] <= 1.60)\n",
    "    \n",
    "    # Assign categories based on conditions\n",
    "    result.loc[dotmc_mask, 'moneyness_category'] = 'DOTMC'\n",
    "    result.loc[otmc_mask, 'moneyness_category'] = 'OTMC'\n",
    "    result.loc[atm_mask, 'moneyness_category'] = 'ATM'\n",
    "    result.loc[otmp_mask, 'moneyness_category'] = 'OTMP'\n",
    "    result.loc[dotmp_mask, 'moneyness_category'] = 'DOTMP'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply moneyness categorization\n",
    "test_data_with_all_predictions = add_moneyness_categories(test_data_with_all_predictions)\n",
    "print(f\"Added moneyness categories to test_data_per_horizon\")\n",
    "    \n",
    "    # Print sample data to verify\n",
    "display_columns = ['cp_flag', 'moneyness', 'moneyness_category']\n",
    "print(f\"\\nSample data for horizon:\")\n",
    "print(test_data_with_all_predictions[display_columns].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05488a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IVRMSE by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS Base   BS NN3   BS NN4  AHBS Base  AHBS NN3  AHBS NN4  CW Base   CW NN3   CW NN4\n",
      "          DOTMC 0.135785 0.011299 0.011230   0.135335  0.011855  0.011922 0.122504 0.014003 0.014875\n",
      "           OTMC 0.134270 0.009091 0.008835   0.126841  0.008905  0.009309 0.125177 0.009858 0.010410\n",
      "            ATM 0.140271 0.010410 0.009755   0.135411  0.010215  0.010211 0.131621 0.010889 0.010996\n",
      "           OTMP 0.131159 0.009846 0.009727   0.129846  0.010106  0.009851 0.128342 0.011020 0.011246\n",
      "          DOTMP 0.195498 0.015319 0.015249   0.142420  0.015224  0.014460 0.134349 0.016785 0.017231\n",
      "\n",
      "================================================================================\n",
      "Improvement Percentage by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS NN3  BS NN4  AHBS NN3  AHBS NN4  CW NN3  CW NN4\n",
      "          DOTMC  91.68%  91.73%    91.24%    91.19%  88.57%  87.86%\n",
      "           OTMC  93.23%  93.42%    92.98%    92.66%  92.12%  91.68%\n",
      "            ATM  92.58%  93.05%    92.46%    92.46%  91.73%  91.65%\n",
      "           OTMP  92.49%  92.58%    92.22%    92.41%  91.41%  91.24%\n",
      "          DOTMP  92.16%  92.20%    89.31%    89.85%  87.51%  87.17%\n",
      "\n",
      "================================================================================\n",
      "Best Model by Moneyness Group and Error Type\n",
      "================================================================================\n",
      "Moneyness Group Error Type Best Model Base IVRMSE Best IVRMSE Improvement % Other Model  Other IVRMSE  Other Improvement %\n",
      "          DOTMC         BS        NN4    0.135785    0.011230        91.73%         NN3      0.011299            91.678606\n",
      "          DOTMC       AHBS        NN3    0.135335    0.011855        91.24%         NN4      0.011922            91.190771\n",
      "          DOTMC         CW        NN3    0.122504    0.014003        88.57%         NN4      0.014875            87.857844\n",
      "           OTMC         BS        NN4    0.134270    0.008835        93.42%         NN3      0.009091            93.229463\n",
      "           OTMC       AHBS        NN3    0.126841    0.008905        92.98%         NN4      0.009309            92.661199\n",
      "           OTMC         CW        NN3    0.125177    0.009858        92.12%         NN4      0.010410            91.683896\n",
      "            ATM         BS        NN4    0.140271    0.009755        93.05%         NN3      0.010410            92.578662\n",
      "            ATM       AHBS        NN4    0.135411    0.010211        92.46%         NN3      0.010215            92.456361\n",
      "            ATM         CW        NN3    0.131621    0.010889        91.73%         NN4      0.010996            91.645957\n",
      "           OTMP         BS        NN4    0.131159    0.009727        92.58%         NN3      0.009846            92.493345\n",
      "           OTMP       AHBS        NN4    0.129846    0.009851        92.41%         NN3      0.010106            92.217237\n",
      "           OTMP         CW        NN3    0.128342    0.011020        91.41%         NN4      0.011246            91.237894\n",
      "          DOTMP         BS        NN4    0.195498    0.015249        92.20%         NN3      0.015319            92.163997\n",
      "          DOTMP       AHBS        NN4    0.142420    0.014460        89.85%         NN3      0.015224            89.310376\n",
      "          DOTMP         CW        NN3    0.134349    0.016785        87.51%         NN4      0.017231            87.174332\n",
      "\n",
      "================================================================================\n",
      "Summary Statistics\n",
      "================================================================================\n",
      "Overall best model distribution: {'NN4': np.int64(8), 'NN3': np.int64(7)}\n",
      "\n",
      "Average improvement by moneyness group:\n",
      "  OTMC: 92.84%\n",
      "  ATM: 92.41%\n",
      "  OTMP: 92.13%\n",
      "  DOTMC: 90.51%\n",
      "  DOTMP: 89.85%\n"
     ]
    }
   ],
   "source": [
    "# Define the moneyness groups\n",
    "moneyness_groups = ['DOTMC', 'OTMC', 'ATM', 'OTMP', 'DOTMP']\n",
    "\n",
    "def analyze_ivrmse_by_moneyness(test_data):\n",
    "\n",
    "    # 1. Group the test data by moneyness\n",
    "    results = {}\n",
    "    for group in moneyness_groups:\n",
    "        group_data = test_data[test_data['moneyness_category'] == group]\n",
    "        \n",
    "        # Skip if no data in this group\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"Warning: No data found for moneyness group {group}\")\n",
    "            continue\n",
    "            \n",
    "        # Initialize results for this group\n",
    "        results[group] = {}\n",
    "        \n",
    "        # 2. Calculate IVRMSE for each error type and model\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Column names\n",
    "            orig_col = f'iv_{error_type}'\n",
    "            \n",
    "            # Skip if column doesn't exist\n",
    "            if orig_col not in group_data.columns:\n",
    "                print(f\"Warning: Column {orig_col} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate baseline IVRMSE\n",
    "            base_rmse = np.sqrt(mean_squared_error(\n",
    "                group_data['impl_volatility'], \n",
    "                group_data[orig_col]\n",
    "            ))\n",
    "            \n",
    "            # Store baseline IVRMSE\n",
    "            results[group][f\"{error_type}_base\"] = base_rmse\n",
    "            \n",
    "            # Calculate corrected IVRMSE for each model\n",
    "            for model in ['NN3', 'NN4']:\n",
    "                corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "                \n",
    "                # Skip if column doesn't exist\n",
    "                if corrected_col not in group_data.columns:\n",
    "                    print(f\"Warning: Column {corrected_col} not found, skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                corrected_rmse = np.sqrt(mean_squared_error(\n",
    "                    group_data['impl_volatility'], \n",
    "                    group_data[corrected_col]\n",
    "                ))\n",
    "                \n",
    "                # Calculate improvement percentage\n",
    "                improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "                \n",
    "                # Store results\n",
    "                results[group][f\"{error_type}_{model}\"] = corrected_rmse\n",
    "                results[group][f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_results_table(results):\n",
    "    # Prepare IVRMSE table data\n",
    "    ivrmse_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if not key.endswith('improvement'):\n",
    "                if key.endswith('base'):\n",
    "                    # Format baseline columns\n",
    "                    error_type = key.split('_')[0].upper()\n",
    "                    row[f\"{error_type} Base\"] = value\n",
    "                else:\n",
    "                    # Format model columns\n",
    "                    parts = key.split('_')\n",
    "                    error_type = parts[0].upper()\n",
    "                    model = parts[1]\n",
    "                    row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        ivrmse_data.append(row)\n",
    "    \n",
    "    # Prepare improvement percentage table data\n",
    "    improvement_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if key.endswith('improvement'):\n",
    "                # Format improvement columns\n",
    "                parts = key.replace('_improvement', '').split('_')\n",
    "                error_type = parts[0].upper()\n",
    "                model = parts[1]\n",
    "                row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        improvement_data.append(row)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    ivrmse_df = pd.DataFrame(ivrmse_data)\n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Sort columns for better readability\n",
    "    ivrmse_cols = ['Moneyness Group']\n",
    "    improvement_cols = ['Moneyness Group']\n",
    "    \n",
    "    for et in ['BS', 'AHBS', 'CW']:\n",
    "        ivrmse_cols.extend([f\"{et} Base\", f\"{et} NN3\", f\"{et} NN4\"])\n",
    "        improvement_cols.extend([f\"{et} NN3\", f\"{et} NN4\"])\n",
    "    \n",
    "    # Reorder columns if they exist\n",
    "    ivrmse_df = ivrmse_df[[col for col in ivrmse_cols if col in ivrmse_df.columns]]\n",
    "    improvement_df = improvement_df[[col for col in improvement_cols if col in improvement_df.columns]]\n",
    "    \n",
    "    return ivrmse_df, improvement_df\n",
    "\n",
    "def find_best_models(results):\n",
    "    best_models = []\n",
    "    \n",
    "    for group, group_results in results.items():\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Get baseline IVRMSE\n",
    "            base_key = f\"{error_type}_base\"\n",
    "            if base_key not in group_results:\n",
    "                continue\n",
    "                \n",
    "            base_rmse = group_results[base_key]\n",
    "            \n",
    "            # Find best model for this error type\n",
    "            models = [m for m in ['NN3', 'NN4'] if f\"{error_type}_{m}\" in group_results]\n",
    "            if not models:\n",
    "                continue\n",
    "            \n",
    "            # Find model with lowest IVRMSE    \n",
    "            best_model = min(models, key=lambda m: group_results[f\"{error_type}_{m}\"])\n",
    "            best_rmse = group_results[f\"{error_type}_{best_model}\"]\n",
    "            improvement = group_results[f\"{error_type}_{best_model}_improvement\"]\n",
    "            \n",
    "            # Also get the values for the other model for comparison\n",
    "            other_models = [m for m in models if m != best_model]\n",
    "            other_model_data = {}\n",
    "            if other_models:\n",
    "                other_model = other_models[0]\n",
    "                other_rmse = group_results[f\"{error_type}_{other_model}\"]\n",
    "                other_improvement = group_results[f\"{error_type}_{other_model}_improvement\"]\n",
    "                other_model_data = {\n",
    "                    f'Other Model': other_model,\n",
    "                    f'Other IVRMSE': other_rmse,\n",
    "                    f'Other Improvement %': other_improvement\n",
    "                }\n",
    "            \n",
    "            model_data = {\n",
    "                'Moneyness Group': group,\n",
    "                'Error Type': error_type.upper(),\n",
    "                'Best Model': best_model,\n",
    "                'Base IVRMSE': base_rmse,\n",
    "                'Best IVRMSE': best_rmse,\n",
    "                'Improvement %': improvement\n",
    "            }\n",
    "            \n",
    "            # Add other model data if available\n",
    "            model_data.update(other_model_data)\n",
    "            \n",
    "            best_models.append(model_data)\n",
    "    \n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "def print_formatted_tables(results):\n",
    "\n",
    "    # Format results into DataFrames\n",
    "    ivrmse_df, improvement_df = format_results_table(results)\n",
    "    \n",
    "    # Format IVRMSE table\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"IVRMSE by Moneyness Group\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ivrmse_df.to_string(index=False))\n",
    "    \n",
    "    # Format improvement table\n",
    "    pd.set_option('display.float_format', '{:.2f}%'.format)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Improvement Percentage by Moneyness Group\")\n",
    "    print(\"=\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    best_models_df = find_best_models(results)\n",
    "    \n",
    "    # Reset float format for mixed table\n",
    "    pd.set_option('display.float_format', None)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Best Model by Moneyness Group and Error Type\")\n",
    "    print(\"=\" * 80)\n",
    "    # Format specific columns\n",
    "    best_models_df['Base IVRMSE'] = best_models_df['Base IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Best IVRMSE'] = best_models_df['Best IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Improvement %'] = best_models_df['Improvement %'].map('{:.2f}%'.format)\n",
    "    print(best_models_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Summary Statistics\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count best model occurrences\n",
    "    model_counts = best_models_df['Best Model'].value_counts()\n",
    "    print(f\"Overall best model distribution: {dict(model_counts)}\")\n",
    "    \n",
    "    # Average improvement by moneyness group\n",
    "    print(\"\\nAverage improvement by moneyness group:\")\n",
    "    # Convert percentage strings to numeric values\n",
    "    best_models_df['Improvement_Numeric'] = pd.to_numeric(best_models_df['Improvement %'].str.rstrip('%'))\n",
    "    \n",
    "    # Group and calculate means\n",
    "    avg_improvement = best_models_df.groupby('Moneyness Group')['Improvement_Numeric'].mean()\n",
    "    \n",
    "    # Handle the case where there's only one group (which returns a scalar)\n",
    "    if isinstance(avg_improvement, pd.Series):\n",
    "        # Sort if it's a Series with multiple values\n",
    "        sorted_improvements = avg_improvement.sort_values(ascending=False)\n",
    "        for group, imp in sorted_improvements.items():\n",
    "            print(f\"  {group}: {imp:.2f}%\")\n",
    "    else:\n",
    "        # Just print the single value if it's a scalar\n",
    "        group = best_models_df['Moneyness Group'].iloc[0]\n",
    "        print(f\"  {group}: {avg_improvement:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "results = analyze_ivrmse_by_moneyness(test_data_with_all_predictions)\n",
    "print_formatted_tables(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b746bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the table\n",
    "dfm, _ = format_results_table(results)\n",
    "\n",
    "\n",
    "# write to CSV (no index column)\n",
    "dfm.to_csv('results_nn_s_f.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb16b4",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efeb20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most important features for NN3 - BS Error Correction:\n",
      "          Feature  Importance\n",
      "8           theta   20.366203\n",
      "4    strike_price   14.395611\n",
      "1     stock_price   14.167562\n",
      "7            vega   10.933832\n",
      "5           delta    9.789684\n",
      "6           gamma    8.356256\n",
      "3  time_to_expiry    8.183577\n",
      "0         cp_flag    6.330551\n",
      "2       moneyness    5.698659\n",
      "9              rf    1.778068\n",
      "\n",
      "=== Layer Information ===\n",
      "\n",
      "Layer 0: dense_27 (Dense)\n",
      "  Shape: [(10, 128), (128,)]\n",
      "  Weight stats: Mean=0.0458, Std=1.2068\n",
      "  Bias stats: Mean=-0.3419, Std=0.9816\n",
      "\n",
      "Layer 1: dense_28 (Dense)\n",
      "  Shape: [(128, 64), (64,)]\n",
      "  Weight stats: Mean=-0.1313, Std=0.2998\n",
      "  Bias stats: Mean=-0.1656, Std=0.0772\n",
      "\n",
      "Layer 2: dense_29 (Dense)\n",
      "  Shape: [(64, 32), (32,)]\n",
      "  Weight stats: Mean=-0.0744, Std=0.2831\n",
      "  Bias stats: Mean=-0.4025, Std=0.4794\n",
      "\n",
      "Layer 3: dense_30 (Dense)\n",
      "  Shape: [(32, 1), (1,)]\n",
      "  Weight stats: Mean=0.0046, Std=0.2329\n",
      "  Bias stats: Mean=-0.0214, Std=0.0000\n"
     ]
    }
   ],
   "source": [
    "all_models = {\n",
    "    'bs': {},\n",
    "    'ahbs': {},\n",
    "    'cw': {}\n",
    "}\n",
    "\n",
    "def analyze_neural_network_weights(model, feature_names, model_name=\"Neural Network\"):\n",
    "\n",
    "    # Get weights from the first layer\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]  # Shape: (input_features, neurons)\n",
    "    \n",
    "    # Calculate importance as the mean absolute weight for each feature\n",
    "    importance = np.mean(np.abs(first_layer_weights), axis=1)\n",
    "    \n",
    "    # Normalize to sum to 100 for easier interpretation\n",
    "    importance = 100.0 * (importance / np.sum(importance))\n",
    "    \n",
    "    # Create DataFrame with feature names and importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Print the top features\n",
    "    print(f\"Top 20 most important features for {model_name}:\")\n",
    "    print(importance_df.head(20))\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name=\"Neural Network\", top_n=30):\n",
    "\n",
    "    # Select top N features\n",
    "    plot_data = importance_df.head(top_n)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(np.arange(len(plot_data)), plot_data['Importance'], align='center')\n",
    "    plt.yticks(np.arange(len(plot_data)), plot_data['Feature'])\n",
    "    plt.xlabel('Relative Importance (%)')\n",
    "    plt.title(f'Top {top_n} Features - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "    \n",
    "    return plt.gcf()  # Return the figure for saving if needed\n",
    "\n",
    "def calculate_permutation_importance(model, X, y, feature_names, n_repeats=10, random_state=42):\n",
    "\n",
    "    # Function to predict using the neural network\n",
    "    def predict_fn(X_data):\n",
    "        return model.predict(X_data, verbose=0).flatten()\n",
    "    \n",
    "    # Calculate permutation importance using the sklearn function\n",
    "    r = sklearn_permutation_importance(predict_fn, X, y, \n",
    "                                      n_repeats=n_repeats,\n",
    "                                      random_state=random_state)\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': r.importances_mean,\n",
    "        'Std': r.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return perm_importance_df\n",
    "\n",
    "def analyze_deeper_layers(model, layer_indices=None):\n",
    "\n",
    "    if layer_indices is None:\n",
    "        layer_indices = range(len(model.layers))\n",
    "    \n",
    "    layer_info = {}\n",
    "    \n",
    "    for i in layer_indices:\n",
    "        layer = model.layers[i]\n",
    "        weights = layer.get_weights()\n",
    "        \n",
    "        if len(weights) == 0:\n",
    "            continue\n",
    "            \n",
    "        # For each layer, calculate weight statistics\n",
    "        layer_info[i] = {\n",
    "            'name': layer.name,\n",
    "            'type': layer.__class__.__name__,\n",
    "            'shape': [w.shape for w in weights],\n",
    "            'weight_stats': {\n",
    "                'mean': float(np.mean(weights[0])),\n",
    "                'std': float(np.std(weights[0])),\n",
    "                'min': float(np.min(weights[0])),\n",
    "                'max': float(np.max(weights[0])),\n",
    "                'sparsity': float(np.mean(np.abs(weights[0]) < 1e-10))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if len(weights) > 1:  # Has bias\n",
    "            layer_info[i]['bias_stats'] = {\n",
    "                'mean': float(np.mean(weights[1])),\n",
    "                'std': float(np.std(weights[1])),\n",
    "                'min': float(np.min(weights[1])),\n",
    "                'max': float(np.max(weights[1]))\n",
    "            }\n",
    "    \n",
    "    return layer_info\n",
    "\n",
    "def plot_weight_distributions(model, layer_indices=None):\n",
    "\n",
    "    if layer_indices is None:\n",
    "        layer_indices = range(len(model.layers))\n",
    "    \n",
    "    # Filter to keep only layers with weights\n",
    "    layers_with_weights = []\n",
    "    for i in layer_indices:\n",
    "        if len(model.layers[i].get_weights()) > 0:\n",
    "            layers_with_weights.append(i)\n",
    "    \n",
    "    if not layers_with_weights:\n",
    "        print(\"No layers with weights found.\")\n",
    "        return\n",
    "    \n",
    "    n_layers = len(layers_with_weights)\n",
    "    fig, axes = plt.subplots(n_layers, 1, figsize=(10, 3*n_layers))\n",
    "    \n",
    "    if n_layers == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, layer_idx in enumerate(layers_with_weights):\n",
    "        layer = model.layers[layer_idx]\n",
    "        weights = layer.get_weights()[0]\n",
    "        \n",
    "        # Flatten weights for histogram\n",
    "        flat_weights = weights.flatten()\n",
    "        \n",
    "        # Plot histogram\n",
    "        axes[i].hist(flat_weights, bins=50)\n",
    "        axes[i].set_title(f\"Layer {layer_idx}: {layer.name} - Weight Distribution\")\n",
    "        axes[i].set_xlabel(\"Weight Value\")\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_activation_patterns(model, X_sample, layer_indices=None):\n",
    "\n",
    "    if layer_indices is None:\n",
    "        layer_indices = range(len(model.layers))\n",
    "    \n",
    "    activation_models = {}\n",
    "    activation_stats = {}\n",
    "    \n",
    "    for i in layer_indices:\n",
    "        try:\n",
    "            # Create a model that outputs the activations from this layer\n",
    "            activation_model = tf.keras.Model(\n",
    "                inputs=model.input,\n",
    "                outputs=model.layers[i].output\n",
    "            )\n",
    "            \n",
    "            # Get activations\n",
    "            activations = activation_model.predict(X_sample, verbose=0)\n",
    "            \n",
    "            # Calculate activation statistics\n",
    "            activation_stats[i] = {\n",
    "                'layer_name': model.layers[i].name,\n",
    "                'mean': float(np.mean(activations)),\n",
    "                'std': float(np.std(activations)),\n",
    "                'min': float(np.min(activations)),\n",
    "                'max': float(np.max(activations)),\n",
    "                'sparsity': float(np.mean(activations < 1e-10)),\n",
    "                'shape': activations.shape\n",
    "            }\n",
    "        except:\n",
    "            # Skip layers that don't support this analysis\n",
    "            continue\n",
    "    \n",
    "    return activation_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model = all_models['bs']['NN3']\n",
    "    except KeyError:\n",
    "        model = loaded_models['bs']['NN3']\n",
    "    # Get feature names\n",
    "    feature_names = feature_columns\n",
    "    \n",
    "    # 1. Analyze weights and show top features\n",
    "    importance_df = analyze_neural_network_weights(model, feature_names, \"NN3 - BS Error Correction\")\n",
    "    \n",
    "    # 2. Plot feature importance\n",
    "    fig = plot_feature_importance(importance_df, \"NN3 - BS Error Correction\", top_n=30)\n",
    "    plt.savefig(\"NN3_bs_feature_importance.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Analyze deeper layers\n",
    "    layer_info = analyze_deeper_layers(model)\n",
    "    print(\"\\n=== Layer Information ===\")\n",
    "    for layer_idx, info in layer_info.items():\n",
    "        print(f\"\\nLayer {layer_idx}: {info['name']} ({info['type']})\")\n",
    "        print(f\"  Shape: {info['shape']}\")\n",
    "        print(f\"  Weight stats: Mean={info['weight_stats']['mean']:.4f}, \"\n",
    "              f\"Std={info['weight_stats']['std']:.4f}\")\n",
    "        if 'bias_stats' in info:\n",
    "            print(f\"  Bias stats: Mean={info['bias_stats']['mean']:.4f}, \"\n",
    "                  f\"Std={info['bias_stats']['std']:.4f}\")\n",
    "    \n",
    "    # 4. Plot weight distributions\n",
    "    weight_fig = plot_weight_distributions(model)\n",
    "    plt.savefig(\"NN3_bs_weight_distributions.png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b798c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = option_only.sample(n=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d335a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>stock_price</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>time_to_expiry</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>delta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>vega</th>\n",
       "      <th>theta</th>\n",
       "      <th>rf</th>\n",
       "      <th>iv_ahbs</th>\n",
       "      <th>iv_ahbs_error</th>\n",
       "      <th>iv_bs</th>\n",
       "      <th>iv_bs_error</th>\n",
       "      <th>iv_cw</th>\n",
       "      <th>iv_cw_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117086</th>\n",
       "      <td>0.234482</td>\n",
       "      <td>1</td>\n",
       "      <td>153.58500</td>\n",
       "      <td>0.903441</td>\n",
       "      <td>109</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.236069</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>25.841560</td>\n",
       "      <td>-10.622040</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.310503</td>\n",
       "      <td>0.076021</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>0.119316</td>\n",
       "      <td>0.309728</td>\n",
       "      <td>0.075246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859670</th>\n",
       "      <td>0.575511</td>\n",
       "      <td>1</td>\n",
       "      <td>271.44855</td>\n",
       "      <td>0.835226</td>\n",
       "      <td>32</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.136164</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>17.155330</td>\n",
       "      <td>-56.423970</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.352766</td>\n",
       "      <td>-0.222745</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>-0.221713</td>\n",
       "      <td>0.443834</td>\n",
       "      <td>-0.131677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204833</th>\n",
       "      <td>0.257683</td>\n",
       "      <td>1</td>\n",
       "      <td>29.72500</td>\n",
       "      <td>0.928906</td>\n",
       "      <td>232</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.425090</td>\n",
       "      <td>0.063260</td>\n",
       "      <td>9.311382</td>\n",
       "      <td>-2.143478</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.321713</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>0.096115</td>\n",
       "      <td>0.299349</td>\n",
       "      <td>0.041666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549824</th>\n",
       "      <td>0.197926</td>\n",
       "      <td>1</td>\n",
       "      <td>138.72000</td>\n",
       "      <td>0.894968</td>\n",
       "      <td>46</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.064338</td>\n",
       "      <td>0.013023</td>\n",
       "      <td>6.233315</td>\n",
       "      <td>-5.350260</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.333355</td>\n",
       "      <td>0.135429</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>0.155872</td>\n",
       "      <td>0.347881</td>\n",
       "      <td>0.149955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270474</th>\n",
       "      <td>0.396995</td>\n",
       "      <td>0</td>\n",
       "      <td>136.17195</td>\n",
       "      <td>1.184104</td>\n",
       "      <td>172</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-0.219291</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>27.420670</td>\n",
       "      <td>-10.713180</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.317334</td>\n",
       "      <td>-0.079661</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>-0.043197</td>\n",
       "      <td>0.335632</td>\n",
       "      <td>-0.061363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326742</th>\n",
       "      <td>0.605911</td>\n",
       "      <td>0</td>\n",
       "      <td>27.52500</td>\n",
       "      <td>1.019444</td>\n",
       "      <td>78</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.383516</td>\n",
       "      <td>0.048552</td>\n",
       "      <td>4.949155</td>\n",
       "      <td>-6.649651</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.316321</td>\n",
       "      <td>-0.289590</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>-0.252113</td>\n",
       "      <td>0.306363</td>\n",
       "      <td>-0.299548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869960</th>\n",
       "      <td>0.264442</td>\n",
       "      <td>1</td>\n",
       "      <td>234.25500</td>\n",
       "      <td>0.851836</td>\n",
       "      <td>57</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.058229</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>10.505050</td>\n",
       "      <td>-9.113153</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.337809</td>\n",
       "      <td>0.073367</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>0.089356</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>0.115723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295260</th>\n",
       "      <td>0.445018</td>\n",
       "      <td>0</td>\n",
       "      <td>15.75000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>147</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.403819</td>\n",
       "      <td>0.089536</td>\n",
       "      <td>3.753302</td>\n",
       "      <td>-1.872832</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.291049</td>\n",
       "      <td>-0.153969</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>-0.091220</td>\n",
       "      <td>0.305216</td>\n",
       "      <td>-0.139802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822677</th>\n",
       "      <td>0.475291</td>\n",
       "      <td>1</td>\n",
       "      <td>169.41500</td>\n",
       "      <td>0.868795</td>\n",
       "      <td>22</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.153299</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>9.983296</td>\n",
       "      <td>-39.390260</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.350550</td>\n",
       "      <td>-0.124741</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>-0.121493</td>\n",
       "      <td>0.409967</td>\n",
       "      <td>-0.065324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633823</th>\n",
       "      <td>0.304587</td>\n",
       "      <td>0</td>\n",
       "      <td>28.75000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>78</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-0.132590</td>\n",
       "      <td>0.053588</td>\n",
       "      <td>2.842366</td>\n",
       "      <td>-1.827551</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.359939</td>\n",
       "      <td>0.055352</td>\n",
       "      <td>0.353798</td>\n",
       "      <td>0.049211</td>\n",
       "      <td>0.374959</td>\n",
       "      <td>0.070372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         impl_volatility  cp_flag  stock_price  moneyness  time_to_expiry  \\\n",
       "117086          0.234482        1    153.58500   0.903441             109   \n",
       "859670          0.575511        1    271.44855   0.835226              32   \n",
       "204833          0.257683        1     29.72500   0.928906             232   \n",
       "1549824         0.197926        1    138.72000   0.894968              46   \n",
       "1270474         0.396995        0    136.17195   1.184104             172   \n",
       "1326742         0.605911        0     27.52500   1.019444              78   \n",
       "869960          0.264442        1    234.25500   0.851836              57   \n",
       "1295260         0.445018        0     15.75000   1.050000             147   \n",
       "822677          0.475291        1    169.41500   0.868795              22   \n",
       "1633823         0.304587        0     28.75000   1.150000              78   \n",
       "\n",
       "         strike_price     delta     gamma       vega      theta      rf  \\\n",
       "117086          170.0  0.236069  0.015602  25.841560 -10.622040  0.0234   \n",
       "859670          325.0  0.136164  0.004826  17.155330 -56.423970  0.0162   \n",
       "204833           32.0  0.425090  0.063260   9.311382  -2.143478  0.0274   \n",
       "1549824         155.0  0.064338  0.013023   6.233315  -5.350260  0.0369   \n",
       "1270474         115.0 -0.219291  0.008073  27.420670 -10.713180  0.0260   \n",
       "1326742          27.0 -0.383516  0.048552   4.949155  -6.649651  0.0376   \n",
       "869960          275.0  0.058229  0.004818  10.505050  -9.113153  0.0163   \n",
       "1295260          15.0 -0.403819  0.089536   3.753302  -1.872832  0.0304   \n",
       "822677          195.0  0.153299  0.011796   9.983296 -39.390260  0.0107   \n",
       "1633823          25.0 -0.132590  0.053588   2.842366  -1.827551  0.0409   \n",
       "\n",
       "          iv_ahbs  iv_ahbs_error     iv_bs  iv_bs_error     iv_cw  iv_cw_error  \n",
       "117086   0.310503       0.076021  0.353798     0.119316  0.309728     0.075246  \n",
       "859670   0.352766      -0.222745  0.353798    -0.221713  0.443834    -0.131677  \n",
       "204833   0.321713       0.064030  0.353798     0.096115  0.299349     0.041666  \n",
       "1549824  0.333355       0.135429  0.353798     0.155872  0.347881     0.149955  \n",
       "1270474  0.317334      -0.079661  0.353798    -0.043197  0.335632    -0.061363  \n",
       "1326742  0.316321      -0.289590  0.353798    -0.252113  0.306363    -0.299548  \n",
       "869960   0.337809       0.073367  0.353798     0.089356  0.380165     0.115723  \n",
       "1295260  0.291049      -0.153969  0.353798    -0.091220  0.305216    -0.139802  \n",
       "822677   0.350550      -0.124741  0.353798    -0.121493  0.409967    -0.065324  \n",
       "1633823  0.359939       0.055352  0.353798     0.049211  0.374959     0.070372  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d584202",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m y_test = y_test_dict[\u001b[33m'\u001b[39m\u001b[33mbs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Compute permutation importance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m imp_perm = \u001b[43mmanual_permutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[32m     64\u001b[39m title = \u001b[33m\"\u001b[39m\u001b[33mBS NN3 Combined Dataset\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmanual_permutation_importance\u001b[39m\u001b[34m(model, X, y, feature_names, n_repeats, random_state)\u001b[39m\n\u001b[32m     22\u001b[39m X_permuted = X.copy()\n\u001b[32m     23\u001b[39m X_permuted[:, idx] = rng.permutation(X_permuted[:, idx])\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m permuted_preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_permuted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.flatten()\n\u001b[32m     25\u001b[39m score = mean_squared_error(y, permuted_preds)\n\u001b[32m     26\u001b[39m scores.append(score - baseline_score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:560\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    558\u001b[39m callbacks.on_predict_batch_begin(step)\n\u001b[32m    559\u001b[39m data = get_data(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m batch_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m outputs = append_to_outputs(batch_outputs, outputs)\n\u001b[32m    562\u001b[39m callbacks.on_predict_batch_end(step, {\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: batch_outputs})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def manual_permutation_importance(model, X: np.ndarray, y: np.ndarray,\n",
    "                                  feature_names: list[str],\n",
    "                                  n_repeats: int = 10,\n",
    "                                  random_state: int = 42) -> pd.Series:\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    baseline_preds = model.predict(X, verbose=0).flatten()\n",
    "    baseline_score = mean_squared_error(y, baseline_preds)\n",
    "\n",
    "    importances = []\n",
    "    for idx, col in enumerate(feature_names):\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[:, idx] = rng.permutation(X_permuted[:, idx])\n",
    "            permuted_preds = model.predict(X_permuted, verbose=0).flatten()\n",
    "            score = mean_squared_error(y, permuted_preds)\n",
    "            scores.append(score - baseline_score)\n",
    "        importances.append(np.mean(scores))\n",
    "\n",
    "    return pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "def plot_importance(importance: pd.Series, top_n: int = 20, title: str = \"Feature Importance\"):\n",
    "    \"\"\"\n",
    "    Plot horizontal bar chart for the top_n features.\n",
    "    \"\"\"\n",
    "    data = importance.head(top_n).sort_values()\n",
    "    plt.figure(figsize=(8, max(4, 0.2 * top_n)))\n",
    "    data.plot.barh()\n",
    "    plt.xlabel('Importance (Increase in MSE)')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Prepare data once\n",
    "\n",
    "\n",
    "# Load pre-saved feature columns (if different than current feature_names)\n",
    "feature_columns = joblib.load(r'C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\Neural Network SameDay\\Without Characteristics\\feature_columns.pkl')\n",
    "\n",
    "# No manual slicing here\n",
    "_, X_test_scaled, _, y_test_dict, _, feature_columns = prepare_data(sample_df)\n",
    "\n",
    "\n",
    "\n",
    "# Access the specific model you want\n",
    "model = loaded_models['bs']['NN3']\n",
    "\n",
    "# Select the correct target (y) for 'bs'\n",
    "y_test = y_test_dict['bs']\n",
    "\n",
    "# Compute permutation importance\n",
    "imp_perm = manual_permutation_importance(model, X_test_scaled, y_test.values, feature_columns)\n",
    "\n",
    "# Plot the results\n",
    "title = \"BS NN3 Combined Dataset\"\n",
    "plot_importance(imp_perm, top_n=20, title=title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
