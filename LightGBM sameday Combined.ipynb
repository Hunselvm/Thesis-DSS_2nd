{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e9b7fe",
   "metadata": {},
   "source": [
    "Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32fa0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   impl_volatility  cp_flag  stock_price  moneyness  time_to_expiry  \\\n",
      "0         0.210270        1      115.545   0.970966              24   \n",
      "1         0.208124        1      115.545   0.962875              24   \n",
      "2         0.205474        1      115.545   0.954917              24   \n",
      "3         0.278442        0      115.545   1.100429              24   \n",
      "4         0.242212        0      115.545   1.050409              24   \n",
      "\n",
      "   strike_price     delta     gamma       vega      theta  ...  qmj_safety  \\\n",
      "0         119.0  0.329402  0.057909  10.772000 -16.612410  ...    0.820412   \n",
      "1         120.0  0.273000  0.053728   9.892975 -15.153390  ...    0.820412   \n",
      "2         121.0  0.220664  0.048533   8.819048 -13.381580  ...    0.820412   \n",
      "3         105.0 -0.075725  0.017194   4.246497  -9.138635  ...    0.820412   \n",
      "4         110.0 -0.186759  0.037192   7.995862 -15.079300  ...    0.820412   \n",
      "\n",
      "   lag_market_value  row_id  position_in_group   iv_ahbs  iv_ahbs_error  \\\n",
      "0     608960.247515     748                  3  0.346996       0.136726   \n",
      "1     608960.247515     751                  1  0.346099       0.137975   \n",
      "2     608960.247515     775                  5  0.345399       0.139925   \n",
      "3     608960.247515     653                  3  0.386757       0.108315   \n",
      "4     608960.247515     681                  1  0.365726       0.123514   \n",
      "\n",
      "      iv_bs  iv_bs_error     iv_cw  iv_cw_error  \n",
      "0  0.353798     0.143528  0.316948     0.106678  \n",
      "1  0.353798     0.145674  0.318967     0.110843  \n",
      "2  0.353798     0.148324  0.321854     0.116380  \n",
      "3  0.353798     0.075356  0.388674     0.110232  \n",
      "4  0.353798     0.111586  0.342023     0.099811  \n",
      "\n",
      "[5 rows x 168 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import lightgbm as lgb\n",
    "\n",
    "   \n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_train_sameday_real.csv')\n",
    "test_data = pd.read_csv('C:/Users/maxva/OneDrive - Tilburg University/Msc. Data Science/Master Thesis/Data/merged_test_sameday_real.csv')\n",
    "columns_to_remove = ['volume', 'open_interest', 'option_price']\n",
    "test_data = test_data.drop(columns=columns_to_remove)\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "test_data_with_all_predictions = test_data.copy() \n",
    "\n",
    "option_columns = [\n",
    "    'impl_volatility',\n",
    "    'cp_flag',\n",
    "    'moneyness',\n",
    "    'stock_price',\n",
    "    'strike_price',\n",
    "    'time_to_expiry',\n",
    "    'rf',\n",
    "    'iv_ahbs',\n",
    "    'iv_ahbs_error',\n",
    "    'iv_bs',\n",
    "    'iv_bs_error',  # Adjusted from a duplicate iv_ahbs_error\n",
    "    'iv_cw',\n",
    "    'iv_cw_error'\n",
    "]\n",
    "\n",
    "option_only = data\n",
    "print(option_only.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 1: LIGHTGBM MODEL DEFINITION\n",
    "###########################################\n",
    "\n",
    "def create_lgb_model(model_type):\n",
    "\n",
    "    if model_type == 'LGB1':\n",
    "        # Standard LightGBM configuration\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    elif model_type == 'LGB2':\n",
    "        # More complex LightGBM configuration with different hyperparameters\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 63,\n",
    "            'learning_rate': 0.01,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq': 4,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'max_depth': 10,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Choose from 'LGB1' or 'LGB2'.\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "def train_and_evaluate_model(params, X_train, y_train, X_test, y_test, num_boost_round=100):\n",
    "\n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return model, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66054835",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 2: DATA PREPARATION\n",
    "###########################################\n",
    "\n",
    "def prepare_data(option_only):\n",
    "\n",
    "    # Prepare features and target variables\n",
    "    feature_columns = [col for col in option_only.columns if col not in \n",
    "                      ['iv_bs_error', 'iv_ahbs', \"iv_ahbs_error\", \"iv_bs\", \n",
    "                       \"iv_cw\", \"iv_cw_error\", \"impl_volatility\"]]\n",
    "    \n",
    "    # Instead of using train_test_split, use your predefined sets\n",
    "    X_train = option_only[feature_columns]  # Features from your training set\n",
    "    X_test = test_data[feature_columns]  # Features from your test set\n",
    "\n",
    "    # Target variables for each error type\n",
    "    y_bs_train = option_only['iv_bs_error']  \n",
    "    y_bs_test = test_data['iv_bs_error']  \n",
    "\n",
    "    y_ahbs_train = option_only['iv_ahbs_error']  \n",
    "    y_ahbs_test = test_data['iv_ahbs_error'] \n",
    "\n",
    "    y_cw_train = option_only['iv_cw_error']  \n",
    "    y_cw_test = test_data['iv_cw_error'] \n",
    "    \n",
    "    # Initialize a StandardScaler to normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on the training data and transform both training and testing data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create dictionaries to store multiple target variables\n",
    "    y_train_dict = {\n",
    "        'bs': y_bs_train,\n",
    "        'ahbs': y_ahbs_train,\n",
    "        'cw': y_cw_train\n",
    "    }\n",
    "    \n",
    "    y_test_dict = {\n",
    "        'bs': y_bs_test,\n",
    "        'ahbs': y_ahbs_test,\n",
    "        'cw': y_cw_test\n",
    "    }\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_dict, y_test_dict, scaler, feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a205ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 3: PREDICTION FUNCTION\n",
    "###########################################\n",
    "\n",
    "def predict_and_add_to_test_data(models, test_data, scaler, feature_columns, error_type='bs'):\n",
    "\n",
    "    # Create a copy of the test data to avoid modifying the original\n",
    "    result_df = test_data.copy()\n",
    "    \n",
    "    # Extract features from test data\n",
    "    X_test = test_data[feature_columns].values\n",
    "    \n",
    "    # Scale the features using the pre-fitted scaler\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Original value column name\n",
    "    original_column = f'iv_{error_type}'\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Add predictions to the dataframe\n",
    "        column_name = f'iv_{error_type}_pred_{model_name}'\n",
    "        result_df[column_name] = predictions\n",
    "        \n",
    "        # Calculate corrected value by adding the error prediction to the original value\n",
    "        result_df[f'iv_{error_type}_corrected_{model_name}'] = result_df[original_column] - predictions\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 4: FEATURE IMPORTANCE ANALYSIS\n",
    "###########################################\n",
    "\n",
    "def analyze_feature_importance(model, feature_columns):\n",
    "\n",
    "    # Get feature importance\n",
    "    importance = model.feature_importance(importance_type='split')\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1646d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training models for bs error correction ===\n",
      "\n",
      "Training LGB1_bs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000385907\n",
      "LGB1_bs Test MSE: 0.00038590700819387077\n",
      "\n",
      "Training LGB2_bs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.00109553\n",
      "LGB2_bs Test MSE: 0.0010955293396601943\n",
      "\n",
      "=== Training models for ahbs error correction ===\n",
      "\n",
      "Training LGB1_ahbs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000369602\n",
      "LGB1_ahbs Test MSE: 0.0003696019769994083\n",
      "\n",
      "Training LGB2_ahbs...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.00104573\n",
      "LGB2_ahbs Test MSE: 0.0010457262525785094\n",
      "\n",
      "=== Training models for cw error correction ===\n",
      "\n",
      "Training LGB1_cw...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000377346\n",
      "LGB1_cw Test MSE: 0.0003773455601760872\n",
      "\n",
      "Training LGB2_cw...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.000927435\n",
      "LGB2_cw Test MSE: 0.0009274347035356657\n",
      "\n",
      "=== Making predictions for bs error correction ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of test data with bs predictions:\n",
      "      iv_bs  iv_bs_pred_LGB1  iv_bs_corrected_LGB1  iv_bs_pred_LGB2  \\\n",
      "0  0.353798         0.149854              0.203944         0.144318   \n",
      "1  0.353798         0.151848              0.201950         0.147108   \n",
      "2  0.353798         0.151968              0.201830         0.147984   \n",
      "3  0.353798         0.156563              0.197236         0.143736   \n",
      "4  0.353798         0.126640              0.227158         0.128682   \n",
      "\n",
      "   iv_bs_corrected_LGB2  \n",
      "0              0.209480  \n",
      "1              0.206690  \n",
      "2              0.205814  \n",
      "3              0.210062  \n",
      "4              0.225116  \n",
      "\n",
      "=== Making predictions for ahbs error correction ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of test data with ahbs predictions:\n",
      "    iv_ahbs  iv_ahbs_pred_LGB1  iv_ahbs_corrected_LGB1  iv_ahbs_pred_LGB2  \\\n",
      "0  0.350970           0.141861                0.209110           0.141294   \n",
      "1  0.349422           0.143208                0.206213           0.143310   \n",
      "2  0.348100           0.145342                0.202759           0.144491   \n",
      "3  0.344389           0.142541                0.201847           0.135025   \n",
      "4  0.359680           0.126644                0.233036           0.131573   \n",
      "\n",
      "   iv_ahbs_corrected_LGB2  \n",
      "0                0.209676  \n",
      "1                0.206111  \n",
      "2                0.203609  \n",
      "3                0.209363  \n",
      "4                0.228107  \n",
      "\n",
      "=== Making predictions for cw error correction ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of test data with cw predictions:\n",
      "      iv_cw  iv_cw_pred_LGB1  iv_cw_corrected_LGB1  iv_cw_pred_LGB2  \\\n",
      "0  0.316475         0.109987              0.206488         0.108184   \n",
      "1  0.315673         0.110484              0.205189         0.111746   \n",
      "2  0.315839         0.113152              0.202688         0.113891   \n",
      "3  0.341157         0.132083              0.209073         0.133381   \n",
      "4  0.329592         0.092251              0.237341         0.100898   \n",
      "\n",
      "   iv_cw_corrected_LGB2  \n",
      "0              0.208291  \n",
      "1              0.203927  \n",
      "2              0.201948  \n",
      "3              0.207775  \n",
      "4              0.228694  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "# PART 5: COMPLETE WORKFLOW\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Prepare data\n",
    "    X_train_scaled, X_test_scaled, y_train_dict, y_test_dict, scaler, feature_columns = prepare_data(option_only)\n",
    "    \n",
    "    # Step 2: Train models for each target variable\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Dictionary to store all models\n",
    "    all_models = {\n",
    "        'bs': {},\n",
    "        'ahbs': {},\n",
    "        'cw': {}\n",
    "    }\n",
    "    \n",
    "    # Train models for each error type\n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        print(f\"\\n=== Training models for {error_type} error correction ===\")\n",
    "        \n",
    "        # Get the appropriate training and test targets\n",
    "        y_train = y_train_dict[error_type]\n",
    "        y_test = y_test_dict[error_type]\n",
    "        \n",
    "        # Train each model configuration\n",
    "        for lgb_type in ['LGB1', 'LGB2']:\n",
    "            model_name = f\"{lgb_type}_{error_type}\"\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            # Create model parameters\n",
    "            params = create_lgb_model(lgb_type)\n",
    "            \n",
    "            # Train and evaluate model\n",
    "            model, mse = train_and_evaluate_model(\n",
    "                params, X_train_scaled, y_train, X_test_scaled, y_test, num_boost_round=500\n",
    "            )\n",
    "            \n",
    "            # Store model and results\n",
    "            all_models[error_type][lgb_type] = model\n",
    "            results[model_name] = {\n",
    "                'mse': mse,\n",
    "                'feature_importance': analyze_feature_importance(model, feature_columns)\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_name} Test MSE: {mse}\")\n",
    "            \n",
    "            # Save feature importance\n",
    "            results[model_name]['feature_importance'].to_csv(f\"{model_name}_feature_importance_sameday.csv\", index=False)\n",
    "            \n",
    "            # Save model (using LightGBM's save_model method)\n",
    "            model.save_model(f\"{model_name}_model_sameday.txt\")\n",
    "    \n",
    "    # Step 3: Generate predictions on test data for each error type\n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        print(f\"\\n=== Making predictions for {error_type} error correction ===\")\n",
    "        \n",
    "        # Extract the models for this error type\n",
    "        current_models = all_models[error_type]\n",
    "        \n",
    "        # Make predictions - use the accumulated DataFrame\n",
    "        test_data_with_all_predictions = predict_and_add_to_test_data(\n",
    "            current_models, test_data_with_all_predictions, scaler, feature_columns, error_type\n",
    "        )\n",
    "        \n",
    "        # Save intermediate results if desired\n",
    "        test_data_with_all_predictions.to_csv(f'test_data_with_{error_type}_predictions_sameday.csv', index=False)\n",
    "        \n",
    "        # Display sample results from accumulated DataFrame\n",
    "        print(f\"\\nSample of test data with {error_type} predictions:\")\n",
    "        display_columns = ['iv_' + error_type]\n",
    "        for lgb_type in ['LGB1', 'LGB2']:\n",
    "            display_columns.extend([\n",
    "                f'iv_{error_type}_pred_{lgb_type}', \n",
    "                f'iv_{error_type}_corrected_{lgb_type}'\n",
    "            ])\n",
    "        \n",
    "        print(test_data_with_all_predictions[display_columns].head(5))\n",
    "\n",
    "    # Final save with all predictions\n",
    "    test_data_with_all_predictions.to_csv('test_data_with_all_predictions_sameday.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a17a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bs LGB1...\n",
      "✓ Model loaded successfully\n",
      "Loading bs LGB2...\n",
      "✓ Model loaded successfully\n",
      "Loading ahbs LGB1...\n",
      "✓ Model loaded successfully\n",
      "Loading ahbs LGB2...\n",
      "✓ Model loaded successfully\n",
      "Loading cw LGB1...\n",
      "✓ Model loaded successfully\n",
      "Loading cw LGB2...\n",
      "✓ Model loaded successfully\n",
      "\n",
      "=== Model Loading Summary ===\n",
      "bs LGB1: ✓ Loaded\n",
      "bs LGB2: ✓ Loaded\n",
      "ahbs LGB1: ✓ Loaded\n",
      "ahbs LGB2: ✓ Loaded\n",
      "cw LGB1: ✓ Loaded\n",
      "cw LGB2: ✓ Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_lgb_models(model_paths_dict):\n",
    "    loaded_models = {}\n",
    "\n",
    "    for error_type in model_paths_dict:\n",
    "        loaded_models[error_type] = {}\n",
    "\n",
    "        for model_name, path in model_paths_dict[error_type].items():\n",
    "            try:\n",
    "                path = path.replace('\\\\', '/')  # Normalize path\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"✗ {error_type} {model_name}: File not found at {path}\")\n",
    "                    loaded_models[error_type][model_name] = None\n",
    "                    continue\n",
    "\n",
    "                print(f\"Loading {error_type} {model_name}...\")\n",
    "                model = lgb.Booster(model_file=path)\n",
    "                print(f\"✓ Model loaded successfully\")\n",
    "\n",
    "                loaded_models[error_type][model_name] = model\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load {error_type} {model_name}: {str(e)}\")\n",
    "                loaded_models[error_type][model_name] = None\n",
    "\n",
    "    return loaded_models\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model paths dictionary with proper path handling for Windows\n",
    "    # Use forward slashes or raw strings to avoid Unicode escape errors\n",
    "    model_paths_dict = {\n",
    "    'bs': {\n",
    "        'LGB1': r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LightGBM Sameday\\Firm Characteristics\\LGB1_bs_model_sameday.txt\",\n",
    "        'LGB2': r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LightGBM Sameday\\Firm Characteristics\\LGB2_bs_model_sameday.txt\"\n",
    "    },\n",
    "    'ahbs': {\n",
    "        'LGB1': r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LightGBM Sameday\\Firm Characteristics\\LGB1_ahbs_model_sameday.txt\",\n",
    "        'LGB2': r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LightGBM Sameday\\Firm Characteristics\\LGB2_ahbs_model_sameday.txt\"\n",
    "    },\n",
    "    'cw': {\n",
    "        'LGB1': r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LightGBM Sameday\\Firm Characteristics\\LGB1_cw_model_sameday.txt\",\n",
    "        'LGB2': r\"C:\\Users\\maxva\\OneDrive - Tilburg University\\Msc. Data Science\\Master Thesis\\Code\\Models\\LightGBM Sameday\\Firm Characteristics\\LGB2_cw_model_sameday.txt\"\n",
    "    }\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Load all models\n",
    "    loaded_models = load_lgb_models(model_paths_dict)\n",
    "    \n",
    "    # Print summary of loaded models\n",
    "    print(\"\\n=== Model Loading Summary ===\")\n",
    "    for error_type in loaded_models:\n",
    "        for model_name in loaded_models[error_type]:\n",
    "            status = \"✓ Loaded\" if loaded_models[error_type][model_name] is not None else \"✗ Failed\"\n",
    "            print(f\"{error_type} {model_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19a940e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['cp_flag', 'stock_price', 'moneyness', 'time_to_expiry', 'strike_price', 'delta', 'gamma', 'vega', 'theta', 'rf', 'me', 'ff49', 'div12m_me', 'chcsho_12m', 'eqnpo_12m', 'ret_1_0', 'ret_3_1', 'ret_6_1', 'ret_9_1', 'ret_12_1', 'ret_12_7', 'ret_60_12', 'seas_1_1an', 'seas_1_1na', 'seas_2_5an', 'seas_2_5na', 'seas_6_10an', 'seas_6_10na', 'seas_11_15an', 'seas_11_15na', 'at_gr1', 'sale_gr1', 'capx_gr1', 'inv_gr1', 'debt_gr3', 'sale_gr3', 'capx_gr3', 'inv_gr1a', 'lti_gr1a', 'sti_gr1a', 'coa_gr1a', 'col_gr1a', 'cowc_gr1a', 'ncoa_gr1a', 'ncol_gr1a', 'nncoa_gr1a', 'fnl_gr1a', 'nfna_gr1a', 'tax_gr1a', 'be_gr1a', 'ebit_sale', 'gp_at', 'cop_at', 'ope_be', 'ni_be', 'ebit_bev', 'netis_at', 'eqnetis_at', 'dbnetis_at', 'oaccruals_at', 'oaccruals_ni', 'taccruals_at', 'taccruals_ni', 'noa_at', 'opex_at', 'at_turnover', 'sale_bev', 'cash_at', 'sale_emp_gr1', 'emp_gr1', 'ni_inc8q', 'noa_gr1a', 'ppeinv_gr1a', 'lnoa_gr1a', 'capx_gr2', 'saleq_gr1', 'niq_be', 'niq_at', 'niq_be_chg1', 'niq_at_chg1', 'dsale_dinv', 'dsale_drec', 'dgp_dsale', 'dsale_dsga', 'saleq_su', 'niq_su', 'capex_abn', 'op_atl1', 'gp_atl1', 'ope_bel1', 'cop_atl1', 'pi_nix', 'ocf_at', 'op_at', 'ocf_at_chg1', 'at_be', 'ocfq_saleq_std', 'tangibility', 'earnings_variability', 'aliq_at', 'f_score', 'o_score', 'z_score', 'kz_index', 'ni_ar1', 'ni_ivol', 'at_me', 'be_me', 'debt_me', 'netdebt_me', 'sale_me', 'ni_me', 'ocf_me', 'fcf_me', 'eqpo_me', 'eqnpo_me', 'ival_me', 'bev_mev', 'ebitda_mev', 'aliq_mat', 'eq_dur', 'beta_60m', 'resff3_12_1', 'resff3_6_1', 'mispricing_mgmt', 'mispricing_perf', 'ivol_capm_21d', 'iskew_capm_21d', 'coskew_21d', 'beta_dimson_21d', 'ivol_ff3_21d', 'iskew_ff3_21d', 'ivol_hxz4_21d', 'iskew_hxz4_21d', 'rmax5_21d', 'rmax1_21d', 'rvol_21d', 'rskew_21d', 'zero_trades_21d', 'dolvol_126d', 'dolvol_var_126d', 'turnover_126d', 'turnover_var_126d', 'zero_trades_126d', 'zero_trades_252d', 'ami_126d', 'ivol_capm_252d', 'prc_highprc_252d', 'betadown_252d', 'bidaskhl_21d', 'corr_1260d', 'betabab_1260d', 'rmax5_rvol_21d', 'age', 'qmj', 'qmj_prof', 'qmj_growth', 'qmj_safety', 'lag_market_value', 'row_id', 'position_in_group']\n",
      "\n",
      "-- Predicting corrections for BS --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   impl_volatility     iv_bs  iv_bs_pred_LGB1  iv_bs_corrected_LGB1  \\\n",
      "0         0.210270  0.353798         0.153442              0.200356   \n",
      "1         0.208124  0.353798         0.153613              0.200185   \n",
      "2         0.205474  0.353798         0.157860              0.195938   \n",
      "\n",
      "   iv_bs_pred_LGB2  iv_bs_corrected_LGB2  \n",
      "0         0.150535              0.203263  \n",
      "1         0.153409              0.200389  \n",
      "2         0.154192              0.199606  \n",
      "\n",
      "-- Predicting corrections for AHBS --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   impl_volatility   iv_ahbs  iv_ahbs_pred_LGB1  iv_ahbs_corrected_LGB1  \\\n",
      "0         0.210270  0.346996           0.145659                0.201338   \n",
      "1         0.208124  0.346099           0.145598                0.200501   \n",
      "2         0.205474  0.345399           0.143739                0.201660   \n",
      "\n",
      "   iv_ahbs_pred_LGB2  iv_ahbs_corrected_LGB2  \n",
      "0           0.145400                0.201596  \n",
      "1           0.146362                0.199738  \n",
      "2           0.148304                0.197095  \n",
      "\n",
      "-- Predicting corrections for CW --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   impl_volatility     iv_cw  iv_cw_pred_LGB1  iv_cw_corrected_LGB1  \\\n",
      "0         0.210270  0.316948         0.114489              0.202460   \n",
      "1         0.208124  0.318967         0.118588              0.200379   \n",
      "2         0.205474  0.321854         0.117341              0.204514   \n",
      "\n",
      "   iv_cw_pred_LGB2  iv_cw_corrected_LGB2  \n",
      "0         0.115532              0.201417  \n",
      "1         0.120524              0.198443  \n",
      "2         0.119770              0.202084  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Define features (remove target + metadata columns)\n",
    "feature_columns = [col for col in option_only.columns if col not in \n",
    "                    ['iv_bs_error', 'iv_ahbs', \"iv_ahbs_error\", \"iv_bs\", \n",
    "                    \"iv_cw\", \"iv_cw_error\", \"impl_volatility\"]]\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "# 3) Add model predictions and corrections\n",
    "df = option_only\n",
    "for error_type in ['bs', 'ahbs', 'cw']:\n",
    "    print(f\"\\n-- Predicting corrections for {error_type.upper()} --\")\n",
    "    \n",
    "    models_for_type = loaded_models[error_type]  # {'LGB1': model_obj, 'NN4': model_obj}\n",
    "    \n",
    "    # This helper must add pred/corrected cols in-place\n",
    "    df = predict_and_add_to_test_data(\n",
    "        models=models_for_type,\n",
    "        test_data=df,\n",
    "        scaler=scaler,\n",
    "        feature_columns=feature_columns,\n",
    "        error_type=error_type\n",
    "    )\n",
    "    \n",
    "    # Preview new prediction columns\n",
    "    cols = ['impl_volatility', f'iv_{error_type}']\n",
    "    for model in ['LGB1', 'LGB2']:\n",
    "        cols += [f'iv_{error_type}_pred_{model}', f'iv_{error_type}_corrected_{model}']\n",
    "    print(df[cols].head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2b159",
   "metadata": {},
   "source": [
    "Calculate IVRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b089598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IVRMSE Results ===\n",
      "Model           IVRMSE     Improvement \n",
      "----------------------------------------\n",
      "BS              0.150568  (baseline)  \n",
      "bs_LGB1         0.019645  86.95%\n",
      "bs_LGB2         0.033099  78.02%\n",
      "----------------------------------------\n",
      "AHBS            0.134068  (baseline)  \n",
      "ahbs_LGB1       0.019225  85.66%\n",
      "ahbs_LGB2       0.032338  75.88%\n",
      "----------------------------------------\n",
      "CW              0.128764  (baseline)  \n",
      "cw_LGB1         0.019425  84.91%\n",
      "cw_LGB2         0.030454  76.35%\n",
      "----------------------------------------\n",
      "\n",
      "Best overall model: ahbs_LGB1 with IVRMSE = 0.019225\n",
      "Improvement over baseline: 85.66%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_ivrmse(predictions_df, error_types=['bs', 'ahbs', 'cw'], models=['LGB1', 'LGB2']):\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate IVRMSE for each error type and model\n",
    "    for error_type in error_types:\n",
    "        orig_col = f'iv_{error_type}'\n",
    "        \n",
    "        # Calculate base IVRMSE (before correction)\n",
    "        base_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[orig_col]))\n",
    "        results[f\"{error_type}_base\"] = base_rmse\n",
    "        \n",
    "        # Calculate IVRMSE for each model\n",
    "        for model in models:\n",
    "            corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "            \n",
    "            # Skip if corrected column doesn't exist\n",
    "            if corrected_col not in predictions_df.columns:\n",
    "                print(f\"Warning: {corrected_col} column not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate IVRMSE for the corrected predictions\n",
    "            corrected_rmse = np.sqrt(mean_squared_error(predictions_df['impl_volatility'], predictions_df[corrected_col]))\n",
    "            results[f\"{error_type}_{model}\"] = corrected_rmse\n",
    "            \n",
    "            # Calculate improvement percentage\n",
    "            improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "            results[f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the predictions DataFrame (assuming it was saved in the main script)\n",
    "    test_data_with_all_predictions = pd.read_csv('test_data_with_all_predictions_sameday.csv')\n",
    "    \n",
    "    # Calculate IVRMSE\n",
    "    ivrmse_results = calculate_ivrmse(test_data_with_all_predictions)\n",
    "    \n",
    "    # Print results in a table format\n",
    "    print(\"\\n=== IVRMSE Results ===\")\n",
    "    print(f\"{'Model':<15} {'IVRMSE':<10} {'Improvement':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for error_type in ['bs', 'ahbs', 'cw']:\n",
    "        base_key = f\"{error_type}_base\"\n",
    "        if base_key in ivrmse_results:\n",
    "            base_rmse = ivrmse_results[base_key]\n",
    "            print(f\"{error_type.upper():<15} {base_rmse:.6f}  {'(baseline)':<12}\")\n",
    "            \n",
    "            for model in ['LGB1', 'LGB2']:\n",
    "                model_key = f\"{error_type}_{model}\"\n",
    "                imp_key = f\"{error_type}_{model}_improvement\"\n",
    "                \n",
    "                if model_key in ivrmse_results:\n",
    "                    print(f\"{model_key:<15} {ivrmse_results[model_key]:.6f}  {ivrmse_results[imp_key]:.2f}%\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    # Find best overall model\n",
    "    model_keys = [k for k in ivrmse_results.keys() if not k.endswith('base') and not k.endswith('improvement')]\n",
    "    if model_keys:\n",
    "        best_model = min(model_keys, key=lambda k: ivrmse_results[k])\n",
    "        print(f\"\\nBest overall model: {best_model} with IVRMSE = {ivrmse_results[best_model]:.6f}\")\n",
    "        \n",
    "        base_key = f\"{best_model.split('_')[0]}_base\"\n",
    "        imp_key = f\"{best_model}_improvement\"\n",
    "        if base_key in ivrmse_results and imp_key in ivrmse_results:\n",
    "            print(f\"Improvement over baseline: {ivrmse_results[imp_key]:.2f}%\")\n",
    "            \n",
    "    # Export results to CSV for further analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(ivrmse_results.keys()),\n",
    "        'Value': list(ivrmse_results.values())\n",
    "    })\n",
    "    results_df.to_csv('ivrmse_results_lightgbm_sameday.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05488a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IVRMSE by Moneyness Group (Sameday)\n",
      "================================================================================\n",
      "Moneyness Group  BS Base  BS LGB1  BS LGB2  AHBS Base  AHBS LGB1  AHBS LGB2  CW Base  CW LGB1  CW LGB2\n",
      "          DOTMC 0.135785 0.022207 0.036868   0.135335   0.021645   0.035377 0.122504 0.021701 0.030282\n",
      "           OTMC 0.134271 0.017324 0.030313   0.126841   0.016992   0.029372 0.125176 0.017629 0.028439\n",
      "            ATM 0.140269 0.017199 0.029902   0.135411   0.017170   0.029854 0.131622 0.017457 0.030061\n",
      "           OTMP 0.131156 0.017339 0.030115   0.129842   0.017238   0.030923 0.128338 0.017570 0.030008\n",
      "          DOTMP 0.195506 0.023727 0.038147   0.142425   0.022836   0.036510 0.134354 0.022623 0.033389\n",
      "\n",
      "================================================================================\n",
      "Improvement Percentage by Moneyness Group\n",
      "================================================================================\n",
      "Moneyness Group  BS LGB1  BS LGB2  AHBS LGB1  AHBS LGB2  CW LGB1  CW LGB2\n",
      "          DOTMC   83.65%   72.85%     84.01%     73.86%   82.29%   75.28%\n",
      "           OTMC   87.10%   77.42%     86.60%     76.84%   85.92%   77.28%\n",
      "            ATM   87.74%   78.68%     87.32%     77.95%   86.74%   77.16%\n",
      "           OTMP   86.78%   77.04%     86.72%     76.18%   86.31%   76.62%\n",
      "          DOTMP   87.86%   80.49%     83.97%     74.37%   83.16%   75.15%\n",
      "\n",
      "================================================================================\n",
      "Best Model by Moneyness Group and Error Type\n",
      "================================================================================\n",
      "Moneyness Group Error Type Best Model Base IVRMSE Best IVRMSE Improvement % Other Model  Other IVRMSE  Other Improvement %\n",
      "          DOTMC         BS       LGB1    0.135785    0.022207        83.65%        LGB2      0.036868            72.848456\n",
      "          DOTMC       AHBS       LGB1    0.135335    0.021645        84.01%        LGB2      0.035377            73.859768\n",
      "          DOTMC         CW       LGB1    0.122504    0.021701        82.29%        LGB2      0.030282            75.280918\n",
      "           OTMC         BS       LGB1    0.134271    0.017324        87.10%        LGB2      0.030313            77.424249\n",
      "           OTMC       AHBS       LGB1    0.126841    0.016992        86.60%        LGB2      0.029372            76.843243\n",
      "           OTMC         CW       LGB1    0.125176    0.017629        85.92%        LGB2      0.028439            77.280578\n",
      "            ATM         BS       LGB1    0.140269    0.017199        87.74%        LGB2      0.029902            78.682684\n",
      "            ATM       AHBS       LGB1    0.135411    0.017170        87.32%        LGB2      0.029854            77.953283\n",
      "            ATM         CW       LGB1    0.131622    0.017457        86.74%        LGB2      0.030061            77.161303\n",
      "           OTMP         BS       LGB1    0.131156    0.017339        86.78%        LGB2      0.030115            77.038588\n",
      "           OTMP       AHBS       LGB1    0.129842    0.017238        86.72%        LGB2      0.030923            76.184357\n",
      "           OTMP         CW       LGB1    0.128338    0.017570        86.31%        LGB2      0.030008            76.617752\n",
      "          DOTMP         BS       LGB1    0.195506    0.023727        87.86%        LGB2      0.038147            80.487979\n",
      "          DOTMP       AHBS       LGB1    0.142425    0.022836        83.97%        LGB2      0.036510            74.365719\n",
      "          DOTMP         CW       LGB1    0.134354    0.022623        83.16%        LGB2      0.033389            75.148370\n",
      "\n",
      "================================================================================\n",
      "Summary Statistics\n",
      "================================================================================\n",
      "Overall best model distribution: {'LGB1': np.int64(15)}\n",
      "\n",
      "Average improvement by moneyness group:\n",
      "  ATM: 87.27%\n",
      "  OTMP: 86.60%\n",
      "  OTMC: 86.54%\n",
      "  DOTMP: 85.00%\n",
      "  DOTMC: 83.32%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the moneyness groups\n",
    "moneyness_groups = ['DOTMC', 'OTMC', 'ATM', 'OTMP', 'DOTMP']\n",
    "\n",
    "def analyze_ivrmse_by_moneyness(test_data):\n",
    "\n",
    "    # 1. Group the test data by moneyness\n",
    "    results = {}\n",
    "    for group in moneyness_groups:\n",
    "        group_data = test_data[test_data['moneyness_category'] == group]\n",
    "        \n",
    "        # Skip if no data in this group\n",
    "        if len(group_data) == 0:\n",
    "            print(f\"Warning: No data found for moneyness group {group}\")\n",
    "            continue\n",
    "            \n",
    "        # Initialize results for this group\n",
    "        results[group] = {}\n",
    "        \n",
    "        # 2. Calculate IVRMSE for each error type and model\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Column names\n",
    "            orig_col = f'iv_{error_type}'\n",
    "            \n",
    "            # Skip if column doesn't exist\n",
    "            if orig_col not in group_data.columns:\n",
    "                print(f\"Warning: Column {orig_col} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate baseline IVRMSE\n",
    "            base_rmse = np.sqrt(mean_squared_error(\n",
    "                group_data['impl_volatility'], \n",
    "                group_data[orig_col]\n",
    "            ))\n",
    "            \n",
    "            # Store baseline IVRMSE\n",
    "            results[group][f\"{error_type}_base\"] = base_rmse\n",
    "            \n",
    "            # Calculate corrected IVRMSE for each model\n",
    "            for model in ['LGB1', 'LGB2']:\n",
    "                corrected_col = f'iv_{error_type}_corrected_{model}'\n",
    "                \n",
    "                # Skip if column doesn't exist\n",
    "                if corrected_col not in group_data.columns:\n",
    "                    print(f\"Warning: Column {corrected_col} not found, skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                corrected_rmse = np.sqrt(mean_squared_error(\n",
    "                    group_data['impl_volatility'], \n",
    "                    group_data[corrected_col]\n",
    "                ))\n",
    "                \n",
    "                # Calculate improvement percentage\n",
    "                improvement = (base_rmse - corrected_rmse) / base_rmse * 100\n",
    "                \n",
    "                # Store results\n",
    "                results[group][f\"{error_type}_{model}\"] = corrected_rmse\n",
    "                results[group][f\"{error_type}_{model}_improvement\"] = improvement\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_results_table(results):\n",
    "\n",
    "    # Prepare IVRMSE table data\n",
    "    ivrmse_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if not key.endswith('improvement'):\n",
    "                if key.endswith('base'):\n",
    "                    # Format baseline columns\n",
    "                    error_type = key.split('_')[0].upper()\n",
    "                    row[f\"{error_type} Base\"] = value\n",
    "                else:\n",
    "                    # Format model columns\n",
    "                    parts = key.split('_')\n",
    "                    error_type = parts[0].upper()\n",
    "                    model = parts[1]\n",
    "                    row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        ivrmse_data.append(row)\n",
    "    \n",
    "    # Prepare improvement percentage table data\n",
    "    improvement_data = []\n",
    "    for group, group_results in results.items():\n",
    "        row = {'Moneyness Group': group}\n",
    "        \n",
    "        for key, value in group_results.items():\n",
    "            if key.endswith('improvement'):\n",
    "                # Format improvement columns\n",
    "                parts = key.replace('_improvement', '').split('_')\n",
    "                error_type = parts[0].upper()\n",
    "                model = parts[1]\n",
    "                row[f\"{error_type} {model}\"] = value\n",
    "        \n",
    "        improvement_data.append(row)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    ivrmse_df = pd.DataFrame(ivrmse_data)\n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Sort columns for better readability\n",
    "    ivrmse_cols = ['Moneyness Group']\n",
    "    improvement_cols = ['Moneyness Group']\n",
    "    \n",
    "    for et in ['BS', 'AHBS', 'CW']:\n",
    "        ivrmse_cols.extend([f\"{et} Base\", f\"{et} LGB1\", f\"{et} LGB2\"])\n",
    "        improvement_cols.extend([f\"{et} LGB1\", f\"{et} LGB2\"])\n",
    "    \n",
    "    # Reorder columns if they exist\n",
    "    ivrmse_df = ivrmse_df[[col for col in ivrmse_cols if col in ivrmse_df.columns]]\n",
    "    improvement_df = improvement_df[[col for col in improvement_cols if col in improvement_df.columns]]\n",
    "    \n",
    "    return ivrmse_df, improvement_df\n",
    "\n",
    "def find_best_models(results):\n",
    "\n",
    "    best_models = []\n",
    "    \n",
    "    for group, group_results in results.items():\n",
    "        for error_type in ['bs', 'ahbs', 'cw']:\n",
    "            # Get baseline IVRMSE\n",
    "            base_key = f\"{error_type}_base\"\n",
    "            if base_key not in group_results:\n",
    "                continue\n",
    "                \n",
    "            base_rmse = group_results[base_key]\n",
    "            \n",
    "            # Find best model for this error type\n",
    "            models = [m for m in ['LGB1', 'LGB2'] if f\"{error_type}_{m}\" in group_results]\n",
    "            if not models:\n",
    "                continue\n",
    "            \n",
    "            # Find model with lowest IVRMSE    \n",
    "            best_model = min(models, key=lambda m: group_results[f\"{error_type}_{m}\"])\n",
    "            best_rmse = group_results[f\"{error_type}_{best_model}\"]\n",
    "            improvement = group_results[f\"{error_type}_{best_model}_improvement\"]\n",
    "            \n",
    "            # Also get the values for the other model for comparison\n",
    "            other_models = [m for m in models if m != best_model]\n",
    "            other_model_data = {}\n",
    "            if other_models:\n",
    "                other_model = other_models[0]\n",
    "                other_rmse = group_results[f\"{error_type}_{other_model}\"]\n",
    "                other_improvement = group_results[f\"{error_type}_{other_model}_improvement\"]\n",
    "                other_model_data = {\n",
    "                    f'Other Model': other_model,\n",
    "                    f'Other IVRMSE': other_rmse,\n",
    "                    f'Other Improvement %': other_improvement\n",
    "                }\n",
    "            \n",
    "            model_data = {\n",
    "                'Moneyness Group': group,\n",
    "                'Error Type': error_type.upper(),\n",
    "                'Best Model': best_model,\n",
    "                'Base IVRMSE': base_rmse,\n",
    "                'Best IVRMSE': best_rmse,\n",
    "                'Improvement %': improvement\n",
    "            }\n",
    "            \n",
    "            # Add other model data if available\n",
    "            model_data.update(other_model_data)\n",
    "            \n",
    "            best_models.append(model_data)\n",
    "    \n",
    "    return pd.DataFrame(best_models)\n",
    "\n",
    "def print_formatted_tables(results):\n",
    "\n",
    "    # Format results into DataFrames\n",
    "    ivrmse_df, improvement_df = format_results_table(results)\n",
    "    \n",
    "    # Format IVRMSE table\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"IVRMSE by Moneyness Group (Sameday)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(ivrmse_df.to_string(index=False))\n",
    "    \n",
    "    # Format improvement table\n",
    "    pd.set_option('display.float_format', '{:.2f}%'.format)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Improvement Percentage by Moneyness Group\")\n",
    "    print(\"=\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    best_models_df = find_best_models(results)\n",
    "    \n",
    "    # Reset float format for mixed table\n",
    "    pd.set_option('display.float_format', None)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Best Model by Moneyness Group and Error Type\")\n",
    "    print(\"=\" * 80)\n",
    "    # Format specific columns\n",
    "    best_models_df['Base IVRMSE'] = best_models_df['Base IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Best IVRMSE'] = best_models_df['Best IVRMSE'].map('{:.6f}'.format)\n",
    "    best_models_df['Improvement %'] = best_models_df['Improvement %'].map('{:.2f}%'.format)\n",
    "    print(best_models_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Summary Statistics\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count best model occurrences\n",
    "    model_counts = best_models_df['Best Model'].value_counts()\n",
    "    print(f\"Overall best model distribution: {dict(model_counts)}\")\n",
    "    \n",
    "    # Average improvement by moneyness group\n",
    "    print(\"\\nAverage improvement by moneyness group:\")\n",
    "    # Convert percentage strings to numeric values\n",
    "    best_models_df['Improvement_Numeric'] = pd.to_numeric(best_models_df['Improvement %'].str.rstrip('%'))\n",
    "    \n",
    "    # Group and calculate means\n",
    "    avg_improvement = best_models_df.groupby('Moneyness Group')['Improvement_Numeric'].mean()\n",
    "    \n",
    "    # Handle the case where there's only one group (which returns a scalar)\n",
    "    if isinstance(avg_improvement, pd.Series):\n",
    "        # Sort if it's a Series with multiple values\n",
    "        sorted_improvements = avg_improvement.sort_values(ascending=False)\n",
    "        for group, imp in sorted_improvements.items():\n",
    "            print(f\"  {group}: {imp:.2f}%\")\n",
    "    else:\n",
    "        # Just print the single value if it's a scalar\n",
    "        group = best_models_df['Moneyness Group'].iloc[0]\n",
    "        print(f\"  {group}: {avg_improvement:.2f}%\")\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    ivrmse_df.to_csv('ivrmse_by_moneyness_lightgbm.csv', index=False)\n",
    "    improvement_df.to_csv('improvement_by_moneyness_lightgbm.csv', index=False)\n",
    "    best_models_df.to_csv('best_models_by_moneyness_lightgbm.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load test data with predictions\n",
    "    try:\n",
    "        test_data_with_all_predictions = pd.read_csv('test_data_with_all_predictions_sameday.csv')\n",
    "        \n",
    "        # Check if 'moneyness_category' exists, if not, create it\n",
    "        if 'moneyness_category' not in test_data_with_all_predictions.columns:\n",
    "            print(\"Creating moneyness categories...\")\n",
    "            # Define moneyness boundaries\n",
    "            test_data_with_all_predictions['moneyness_category'] = pd.cut(\n",
    "                test_data_with_all_predictions['moneyness'],\n",
    "                bins=[-float('inf'), 0.9, 0.97, 1.03, 1.1, float('inf')],\n",
    "                labels=moneyness_groups\n",
    "            )\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyze_ivrmse_by_moneyness(test_data_with_all_predictions)\n",
    "        print_formatted_tables(results)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Test data file not found. Please run the main script first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
